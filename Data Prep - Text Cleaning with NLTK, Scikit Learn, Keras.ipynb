{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text Manually and with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metamorphosis by Franz Kafka\n",
    "\n",
    "http://www.gutenberg.org/cache/epub/5200/pg5200.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load text\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: Metamorphosis\\n\\nAuthor: Franz Kafka\\n\\nTranslator: David Wyllie\\n\\nRelease Date: August 16, 2005 [EBook #5200]\\nFirst posted: May 13, 2002\\nLast updated: May 20, 2012\\n\\nLanguage: English\\n\\n\\n*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\\n\\n\\n\\n\\nCopyright (C) 2002 David Wyllie.\\n\\n\\n\\n\\n\\n  Metamorphosis\\n  Franz Kafka\\n\\nTranslated by David Wyllie\\n\\n\\n\\nI\\n\\n\\nOne morning, when Gregor Samsa woke from troubled dreams, he found\\nhimself transformed in his bed into a horrible vermin.  He lay on\\nhis armour-l'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title:', 'Metamorphosis', 'Author:', 'Franz', 'Kafka', 'Translator:', 'David', 'Wyllie', 'Release', 'Date:', 'August', '16,', '2005', '[EBook', '#5200]', 'First', 'posted:', 'May', '13,', '2002', 'Last', 'updated:', 'May', '20,', '2012', 'Language:', 'English', '***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', '***', 'Copyright', '(C)', '2002', 'David', 'Wyllie.', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning,', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams,', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin.', 'He', 'lay', 'on', 'his', 'armour-like', 'back,', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly,', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections.', 'The', 'bedding']\n"
     ]
    }
   ],
   "source": [
    "# split into words by white space\n",
    "words = text.split()\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated', 'May', '20', '2012', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright', 'C', '2002', 'David', 'Wyllie', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was']\n"
     ]
    }
   ],
   "source": [
    "#Another approach might be to use the regex model (re) and split the document into words by\n",
    "#selecting for strings of alphanumeric characters (a-z, A-Z, 0-9 and ` ').\n",
    "\n",
    "import re\n",
    "# split based on words only\n",
    "words = re.split(r'\\W+', text)\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', '16', '2005', 'EBook', '5200', 'First', 'posted', 'May', '13', '2002', 'Last', 'updated', 'May', '20', '2012', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright', 'C', '2002', 'David', 'Wyllie', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in words]\n",
    "print(stripped[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title',\n",
       " 'Metamorphosis',\n",
       " 'Author',\n",
       " 'Franz',\n",
       " 'Kafka',\n",
       " 'Translator',\n",
       " 'David',\n",
       " 'Wyllie',\n",
       " 'Release',\n",
       " 'Date',\n",
       " 'August',\n",
       " '16',\n",
       " '2005',\n",
       " 'EBook',\n",
       " '5200',\n",
       " 'First',\n",
       " 'posted',\n",
       " 'May',\n",
       " '13',\n",
       " '2002',\n",
       " 'Last',\n",
       " 'updated',\n",
       " 'May',\n",
       " '20',\n",
       " '2012',\n",
       " 'Language',\n",
       " 'English',\n",
       " 'START',\n",
       " 'OF',\n",
       " 'THIS',\n",
       " 'PROJECT',\n",
       " 'GUTENBERG',\n",
       " 'EBOOK',\n",
       " 'METAMORPHOSIS',\n",
       " 'Copyright',\n",
       " 'C',\n",
       " '2002',\n",
       " 'David',\n",
       " 'Wyllie',\n",
       " 'Metamorphosis',\n",
       " 'Franz',\n",
       " 'Kafka',\n",
       " 'Translated',\n",
       " 'by',\n",
       " 'David',\n",
       " 'Wyllie',\n",
       " 'I',\n",
       " 'One',\n",
       " 'morning',\n",
       " 'when',\n",
       " 'Gregor',\n",
       " 'Samsa',\n",
       " 'woke',\n",
       " 'from',\n",
       " 'troubled',\n",
       " 'dreams',\n",
       " 'he',\n",
       " 'found',\n",
       " 'himself',\n",
       " 'transformed',\n",
       " 'in',\n",
       " 'his',\n",
       " 'bed',\n",
       " 'into',\n",
       " 'a',\n",
       " 'horrible',\n",
       " 'vermin',\n",
       " 'He',\n",
       " 'lay',\n",
       " 'on',\n",
       " 'his',\n",
       " 'armour',\n",
       " 'like',\n",
       " 'back',\n",
       " 'and',\n",
       " 'if',\n",
       " 'he',\n",
       " 'lifted',\n",
       " 'his',\n",
       " 'head',\n",
       " 'a',\n",
       " 'little',\n",
       " 'he',\n",
       " 'could',\n",
       " 'see',\n",
       " 'his',\n",
       " 'brown',\n",
       " 'belly',\n",
       " 'slightly',\n",
       " 'domed',\n",
       " 'and',\n",
       " 'divided',\n",
       " 'by',\n",
       " 'arches',\n",
       " 'into',\n",
       " 'stiff',\n",
       " 'sections',\n",
       " 'The',\n",
       " 'bedding',\n",
       " 'was',\n",
       " 'hardly',\n",
       " 'able',\n",
       " 'to',\n",
       " 'cover',\n",
       " 'it',\n",
       " 'and',\n",
       " 'seemed',\n",
       " 'ready',\n",
       " 'to',\n",
       " 'slide',\n",
       " 'off',\n",
       " 'any',\n",
       " 'moment',\n",
       " 'His',\n",
       " 'many',\n",
       " 'legs',\n",
       " 'pitifully',\n",
       " 'thin',\n",
       " 'compared',\n",
       " 'with',\n",
       " 'the',\n",
       " 'size',\n",
       " 'of',\n",
       " 'the',\n",
       " 'rest',\n",
       " 'of',\n",
       " 'him',\n",
       " 'waved',\n",
       " 'about',\n",
       " 'helplessly',\n",
       " 'as',\n",
       " 'he',\n",
       " 'looked',\n",
       " 'What',\n",
       " 's',\n",
       " 'happened',\n",
       " 'to',\n",
       " 'me',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'It',\n",
       " 'wasn',\n",
       " 't',\n",
       " 'a',\n",
       " 'dream',\n",
       " 'His',\n",
       " 'room',\n",
       " 'a',\n",
       " 'proper',\n",
       " 'human',\n",
       " 'room',\n",
       " 'although',\n",
       " 'a',\n",
       " 'little',\n",
       " 'too',\n",
       " 'small',\n",
       " 'lay',\n",
       " 'peacefully',\n",
       " 'between',\n",
       " 'its',\n",
       " 'four',\n",
       " 'familiar',\n",
       " 'walls',\n",
       " 'A',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'textile',\n",
       " 'samples',\n",
       " 'lay',\n",
       " 'spread',\n",
       " 'out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'table',\n",
       " 'Samsa',\n",
       " 'was',\n",
       " 'a',\n",
       " 'travelling',\n",
       " 'salesman',\n",
       " 'and',\n",
       " 'above',\n",
       " 'it',\n",
       " 'there',\n",
       " 'hung',\n",
       " 'a',\n",
       " 'picture',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " 'recently',\n",
       " 'cut',\n",
       " 'out',\n",
       " 'of',\n",
       " 'an',\n",
       " 'illustrated',\n",
       " 'magazine',\n",
       " 'and',\n",
       " 'housed',\n",
       " 'in',\n",
       " 'a',\n",
       " 'nice',\n",
       " 'gilded',\n",
       " 'frame',\n",
       " 'It',\n",
       " 'showed',\n",
       " 'a',\n",
       " 'lady',\n",
       " 'fitted',\n",
       " 'out',\n",
       " 'with',\n",
       " 'a',\n",
       " 'fur',\n",
       " 'hat',\n",
       " 'and',\n",
       " 'fur',\n",
       " 'boa',\n",
       " 'who',\n",
       " 'sat',\n",
       " 'upright',\n",
       " 'raising',\n",
       " 'a',\n",
       " 'heavy',\n",
       " 'fur',\n",
       " 'muff',\n",
       " 'that',\n",
       " 'covered',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'of',\n",
       " 'her',\n",
       " 'lower',\n",
       " 'arm',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'viewer',\n",
       " 'Gregor',\n",
       " 'then',\n",
       " 'turned',\n",
       " 'to',\n",
       " 'look',\n",
       " 'out',\n",
       " 'the',\n",
       " 'window',\n",
       " 'at',\n",
       " 'the',\n",
       " 'dull',\n",
       " 'weather',\n",
       " 'Drops',\n",
       " 'of',\n",
       " 'rain',\n",
       " 'could',\n",
       " 'be',\n",
       " 'heard',\n",
       " 'hitting',\n",
       " 'the',\n",
       " 'pane',\n",
       " 'which',\n",
       " 'made',\n",
       " 'him',\n",
       " 'feel',\n",
       " 'quite',\n",
       " 'sad',\n",
       " 'How',\n",
       " 'about',\n",
       " 'if',\n",
       " 'I',\n",
       " 'sleep',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'longer',\n",
       " 'and',\n",
       " 'forget',\n",
       " 'all',\n",
       " 'this',\n",
       " 'nonsense',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'but',\n",
       " 'that',\n",
       " 'was',\n",
       " 'something',\n",
       " 'he',\n",
       " 'was',\n",
       " 'unable',\n",
       " 'to',\n",
       " 'do',\n",
       " 'because',\n",
       " 'he',\n",
       " 'was',\n",
       " 'used',\n",
       " 'to',\n",
       " 'sleeping',\n",
       " 'on',\n",
       " 'his',\n",
       " 'right',\n",
       " 'and',\n",
       " 'in',\n",
       " 'his',\n",
       " 'present',\n",
       " 'state',\n",
       " 'couldn',\n",
       " 't',\n",
       " 'get',\n",
       " 'into',\n",
       " 'that',\n",
       " 'position',\n",
       " 'However',\n",
       " 'hard',\n",
       " 'he',\n",
       " 'threw',\n",
       " 'himself',\n",
       " 'onto',\n",
       " 'his',\n",
       " 'right',\n",
       " 'he',\n",
       " 'always',\n",
       " 'rolled',\n",
       " 'back',\n",
       " 'to',\n",
       " 'where',\n",
       " 'he',\n",
       " 'was',\n",
       " 'He',\n",
       " 'must',\n",
       " 'have',\n",
       " 'tried',\n",
       " 'it',\n",
       " 'a',\n",
       " 'hundred',\n",
       " 'times',\n",
       " 'shut',\n",
       " 'his',\n",
       " 'eyes',\n",
       " 'so',\n",
       " 'that',\n",
       " 'he',\n",
       " 'wouldn',\n",
       " 't',\n",
       " 'have',\n",
       " 'to',\n",
       " 'look',\n",
       " 'at',\n",
       " 'the',\n",
       " 'floundering',\n",
       " 'legs',\n",
       " 'and',\n",
       " 'only',\n",
       " 'stopped',\n",
       " 'when',\n",
       " 'he',\n",
       " 'began',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'a',\n",
       " 'mild',\n",
       " 'dull',\n",
       " 'pain',\n",
       " 'there',\n",
       " 'that',\n",
       " 'he',\n",
       " 'had',\n",
       " 'never',\n",
       " 'felt',\n",
       " 'before',\n",
       " 'Oh',\n",
       " 'God',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'what',\n",
       " 'a',\n",
       " 'strenuous',\n",
       " 'career',\n",
       " 'it',\n",
       " 'is',\n",
       " 'that',\n",
       " 'I',\n",
       " 've',\n",
       " 'chosen',\n",
       " 'Travelling',\n",
       " 'day',\n",
       " 'in',\n",
       " 'and',\n",
       " 'day',\n",
       " 'out',\n",
       " 'Doing',\n",
       " 'business',\n",
       " 'like',\n",
       " 'this',\n",
       " 'takes',\n",
       " 'much',\n",
       " 'more',\n",
       " 'effort',\n",
       " 'than',\n",
       " 'doing',\n",
       " 'your',\n",
       " 'own',\n",
       " 'business',\n",
       " 'at',\n",
       " 'home',\n",
       " 'and',\n",
       " 'on',\n",
       " 'top',\n",
       " 'of',\n",
       " 'that',\n",
       " 'there',\n",
       " 's',\n",
       " 'the',\n",
       " 'curse',\n",
       " 'of',\n",
       " 'travelling',\n",
       " 'worries',\n",
       " 'about',\n",
       " 'making',\n",
       " 'train',\n",
       " 'connections',\n",
       " 'bad',\n",
       " 'and',\n",
       " 'irregular',\n",
       " 'food',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'different',\n",
       " 'people',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'so',\n",
       " 'that',\n",
       " 'you',\n",
       " 'can',\n",
       " 'never',\n",
       " 'get',\n",
       " 'to',\n",
       " 'know',\n",
       " 'anyone',\n",
       " 'or',\n",
       " 'become',\n",
       " 'friendly',\n",
       " 'with',\n",
       " 'them',\n",
       " 'It',\n",
       " 'can',\n",
       " 'all',\n",
       " 'go',\n",
       " 'to',\n",
       " 'Hell',\n",
       " 'He',\n",
       " 'felt',\n",
       " 'a',\n",
       " 'slight',\n",
       " 'itch',\n",
       " 'up',\n",
       " 'on',\n",
       " 'his',\n",
       " 'belly',\n",
       " 'pushed',\n",
       " 'himself',\n",
       " 'slowly',\n",
       " 'up',\n",
       " 'on',\n",
       " 'his',\n",
       " 'back',\n",
       " 'towards',\n",
       " 'the',\n",
       " 'headboard',\n",
       " 'so',\n",
       " 'that',\n",
       " 'he',\n",
       " 'could',\n",
       " 'lift',\n",
       " 'his',\n",
       " 'head',\n",
       " 'better',\n",
       " 'found',\n",
       " 'where',\n",
       " 'the',\n",
       " 'itch',\n",
       " 'was',\n",
       " 'and',\n",
       " 'saw',\n",
       " 'that',\n",
       " 'it',\n",
       " 'was',\n",
       " 'covered',\n",
       " 'with',\n",
       " 'lots',\n",
       " 'of',\n",
       " 'little',\n",
       " 'white',\n",
       " 'spots',\n",
       " 'which',\n",
       " 'he',\n",
       " 'didn',\n",
       " 't',\n",
       " 'know',\n",
       " 'what',\n",
       " 'to',\n",
       " 'make',\n",
       " 'of',\n",
       " 'and',\n",
       " 'when',\n",
       " 'he',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'feel',\n",
       " 'the',\n",
       " 'place',\n",
       " 'with',\n",
       " 'one',\n",
       " 'of',\n",
       " 'his',\n",
       " 'legs',\n",
       " 'he',\n",
       " 'drew',\n",
       " 'it',\n",
       " 'quickly',\n",
       " 'back',\n",
       " 'because',\n",
       " 'as',\n",
       " 'soon',\n",
       " 'as',\n",
       " 'he',\n",
       " 'touched',\n",
       " 'it',\n",
       " 'he',\n",
       " 'was',\n",
       " 'overcome',\n",
       " 'by',\n",
       " 'a',\n",
       " 'cold',\n",
       " 'shudder',\n",
       " 'He',\n",
       " 'slid',\n",
       " 'back',\n",
       " 'into',\n",
       " 'his',\n",
       " 'former',\n",
       " 'position',\n",
       " 'Getting',\n",
       " 'up',\n",
       " 'early',\n",
       " 'all',\n",
       " 'the',\n",
       " 'time',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'makes',\n",
       " 'you',\n",
       " 'stupid',\n",
       " 'You',\n",
       " 've',\n",
       " 'got',\n",
       " 'to',\n",
       " 'get',\n",
       " 'enough',\n",
       " 'sleep',\n",
       " 'Other',\n",
       " 'travelling',\n",
       " 'salesmen',\n",
       " 'live',\n",
       " 'a',\n",
       " 'life',\n",
       " 'of',\n",
       " 'luxury',\n",
       " 'For',\n",
       " 'instance',\n",
       " 'whenever',\n",
       " 'I',\n",
       " 'go',\n",
       " 'back',\n",
       " 'to',\n",
       " 'the',\n",
       " 'guest',\n",
       " 'house',\n",
       " 'during',\n",
       " 'the',\n",
       " 'morning',\n",
       " 'to',\n",
       " 'copy',\n",
       " 'out',\n",
       " 'the',\n",
       " 'contract',\n",
       " 'these',\n",
       " 'gentlemen',\n",
       " 'are',\n",
       " 'always',\n",
       " 'still',\n",
       " 'sitting',\n",
       " 'there',\n",
       " 'eating',\n",
       " 'their',\n",
       " 'breakfasts',\n",
       " 'I',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'just',\n",
       " 'try',\n",
       " 'that',\n",
       " 'with',\n",
       " 'my',\n",
       " 'boss',\n",
       " 'I',\n",
       " 'd',\n",
       " 'get',\n",
       " 'kicked',\n",
       " 'out',\n",
       " 'on',\n",
       " 'the',\n",
       " 'spot',\n",
       " 'But',\n",
       " 'who',\n",
       " 'knows',\n",
       " 'maybe',\n",
       " 'that',\n",
       " 'would',\n",
       " 'be',\n",
       " 'the',\n",
       " 'best',\n",
       " 'thing',\n",
       " 'for',\n",
       " 'me',\n",
       " 'If',\n",
       " 'I',\n",
       " 'didn',\n",
       " 't',\n",
       " 'have',\n",
       " 'my',\n",
       " 'parents',\n",
       " 'to',\n",
       " 'think',\n",
       " 'about',\n",
       " 'I',\n",
       " 'd',\n",
       " 'have',\n",
       " 'given',\n",
       " 'in',\n",
       " 'my',\n",
       " 'notice',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'I',\n",
       " 'd',\n",
       " 'have',\n",
       " 'gone',\n",
       " 'up',\n",
       " 'to',\n",
       " 'the',\n",
       " 'boss',\n",
       " 'and',\n",
       " 'told',\n",
       " 'him',\n",
       " 'just',\n",
       " 'what',\n",
       " 'I',\n",
       " 'think',\n",
       " 'tell',\n",
       " 'him',\n",
       " 'everything',\n",
       " 'I',\n",
       " 'would',\n",
       " 'let',\n",
       " 'him',\n",
       " 'know',\n",
       " 'just',\n",
       " 'what',\n",
       " 'I',\n",
       " 'feel',\n",
       " 'He',\n",
       " 'd',\n",
       " 'fall',\n",
       " 'right',\n",
       " 'off',\n",
       " 'his',\n",
       " 'desk',\n",
       " 'And',\n",
       " 'it',\n",
       " 's',\n",
       " 'a',\n",
       " 'funny',\n",
       " 'sort',\n",
       " 'of',\n",
       " 'business',\n",
       " 'to',\n",
       " 'be',\n",
       " 'sitting',\n",
       " 'up',\n",
       " 'there',\n",
       " 'at',\n",
       " 'your',\n",
       " 'desk',\n",
       " 'talking',\n",
       " 'down',\n",
       " 'at',\n",
       " 'your',\n",
       " 'subordinates',\n",
       " 'from',\n",
       " 'up',\n",
       " 'there',\n",
       " 'especially',\n",
       " 'when',\n",
       " 'you',\n",
       " 'have',\n",
       " 'to',\n",
       " 'go',\n",
       " 'right',\n",
       " 'up',\n",
       " 'close',\n",
       " 'because',\n",
       " 'the',\n",
       " 'boss',\n",
       " 'is',\n",
       " 'hard',\n",
       " 'of',\n",
       " 'hearing',\n",
       " 'Well',\n",
       " 'there',\n",
       " 's',\n",
       " 'still',\n",
       " 'some',\n",
       " 'hope',\n",
       " 'once',\n",
       " 'I',\n",
       " 've',\n",
       " 'got',\n",
       " 'the',\n",
       " 'money',\n",
       " 'together',\n",
       " 'to',\n",
       " 'pay',\n",
       " 'off',\n",
       " 'my',\n",
       " 'parents',\n",
       " 'debt',\n",
       " 'to',\n",
       " 'him',\n",
       " 'another',\n",
       " 'five',\n",
       " 'or',\n",
       " 'six',\n",
       " 'years',\n",
       " 'I',\n",
       " 'suppose',\n",
       " 'that',\n",
       " 's',\n",
       " 'definitely',\n",
       " 'what',\n",
       " 'I',\n",
       " 'll',\n",
       " 'do',\n",
       " 'That',\n",
       " 's',\n",
       " 'when',\n",
       " 'I',\n",
       " 'll',\n",
       " 'make',\n",
       " 'the',\n",
       " 'big',\n",
       " 'change',\n",
       " 'First',\n",
       " 'of',\n",
       " 'all',\n",
       " 'though',\n",
       " 'I',\n",
       " 've',\n",
       " 'got',\n",
       " 'to',\n",
       " 'get',\n",
       " 'up',\n",
       " 'my',\n",
       " 'train',\n",
       " 'leaves',\n",
       " 'at',\n",
       " 'five',\n",
       " 'And',\n",
       " 'he',\n",
       " 'looked',\n",
       " 'over',\n",
       " 'at',\n",
       " 'the',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'ticking',\n",
       " 'on',\n",
       " 'the',\n",
       " 'chest',\n",
       " 'of',\n",
       " 'drawers',\n",
       " 'God',\n",
       " 'in',\n",
       " 'Heaven',\n",
       " 'he',\n",
       " 'thought',\n",
       " 'It',\n",
       " 'was',\n",
       " 'half',\n",
       " 'past',\n",
       " 'six',\n",
       " 'and',\n",
       " 'the',\n",
       " 'hands',\n",
       " 'were',\n",
       " 'quietly',\n",
       " 'moving',\n",
       " 'forwards',\n",
       " 'it',\n",
       " 'was',\n",
       " 'even',\n",
       " 'later',\n",
       " 'than',\n",
       " 'half',\n",
       " 'past',\n",
       " 'more',\n",
       " 'like',\n",
       " 'quarter',\n",
       " 'to',\n",
       " 'seven',\n",
       " 'Had',\n",
       " 'the',\n",
       " 'alarm',\n",
       " 'clock',\n",
       " 'not',\n",
       " 'rung',\n",
       " 'He',\n",
       " 'could',\n",
       " 'see',\n",
       " 'from',\n",
       " 'the',\n",
       " 'bed',\n",
       " 'that',\n",
       " 'it',\n",
       " 'had',\n",
       " 'been',\n",
       " 'set',\n",
       " 'for',\n",
       " 'four',\n",
       " 'o',\n",
       " 'clock',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'have',\n",
       " 'been',\n",
       " 'it',\n",
       " 'certainly',\n",
       " 'must',\n",
       " 'have',\n",
       " 'rung',\n",
       " 'Yes',\n",
       " 'but',\n",
       " 'was',\n",
       " 'it',\n",
       " 'possible',\n",
       " 'to',\n",
       " 'quietly',\n",
       " 'sleep',\n",
       " 'through',\n",
       " 'that',\n",
       " 'furniture',\n",
       " 'rattling',\n",
       " 'noise',\n",
       " 'True',\n",
       " 'he',\n",
       " 'had',\n",
       " 'not',\n",
       " 'slept',\n",
       " 'peacefully',\n",
       " 'but',\n",
       " 'probably',\n",
       " 'all',\n",
       " 'the',\n",
       " 'more',\n",
       " 'deeply',\n",
       " 'because',\n",
       " 'of',\n",
       " 'that',\n",
       " 'What',\n",
       " 'should',\n",
       " 'he',\n",
       " 'do',\n",
       " 'now',\n",
       " 'The',\n",
       " 'next',\n",
       " 'train',\n",
       " 'went',\n",
       " 'at',\n",
       " 'seven',\n",
       " 'if',\n",
       " 'he',\n",
       " 'were',\n",
       " 'to',\n",
       " 'catch',\n",
       " 'that',\n",
       " 'he',\n",
       " 'would',\n",
       " 'have',\n",
       " 'to',\n",
       " 'rush',\n",
       " 'like',\n",
       " 'mad',\n",
       " 'and',\n",
       " 'the',\n",
       " 'collection',\n",
       " 'of',\n",
       " 'samples',\n",
       " 'was',\n",
       " 'still',\n",
       " 'not',\n",
       " 'packed',\n",
       " 'and',\n",
       " 'he',\n",
       " 'did',\n",
       " 'not',\n",
       " 'at',\n",
       " 'all',\n",
       " 'feel',\n",
       " 'particularly',\n",
       " 'fresh',\n",
       " 'and',\n",
       " 'lively',\n",
       " 'And',\n",
       " 'even',\n",
       " 'if',\n",
       " 'he',\n",
       " 'did',\n",
       " 'catch',\n",
       " 'the',\n",
       " 'train',\n",
       " 'he',\n",
       " 'would',\n",
       " 'not',\n",
       " 'avoid',\n",
       " 'his',\n",
       " 'boss',\n",
       " 's',\n",
       " 'anger',\n",
       " 'as',\n",
       " 'the',\n",
       " 'office',\n",
       " 'assistant',\n",
       " 'would',\n",
       " 'have',\n",
       " 'been',\n",
       " 'there',\n",
       " 'to',\n",
       " 'see',\n",
       " 'the',\n",
       " 'five',\n",
       " 'o',\n",
       " 'clock',\n",
       " 'train',\n",
       " 'go',\n",
       " 'he',\n",
       " 'would',\n",
       " 'have',\n",
       " 'put',\n",
       " 'in',\n",
       " 'his',\n",
       " 'report',\n",
       " 'about',\n",
       " 'Gregor',\n",
       " 's',\n",
       " 'not',\n",
       " 'being',\n",
       " 'there',\n",
       " 'a',\n",
       " 'long',\n",
       " 'time',\n",
       " 'ago',\n",
       " 'The',\n",
       " 'office',\n",
       " 'assistant',\n",
       " 'was',\n",
       " 'the',\n",
       " 'boss',\n",
       " 's',\n",
       " 'man',\n",
       " 'spineless',\n",
       " 'and',\n",
       " 'with',\n",
       " 'no',\n",
       " 'understanding',\n",
       " 'What',\n",
       " 'about',\n",
       " 'if',\n",
       " 'he',\n",
       " 'reported',\n",
       " 'sick',\n",
       " 'But',\n",
       " 'that',\n",
       " 'would',\n",
       " 'be',\n",
       " 'extremely',\n",
       " 'strained',\n",
       " 'and',\n",
       " 'suspicious',\n",
       " 'as',\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can use a similar approach to filter out all non-printable characters by selecting the inverse of the string.printable constant.\n",
    "re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "result = [re_print.sub('', w) for w in words]\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', '16', '2005', 'ebook', '5200', 'first', 'posted', 'may', '13', '2002', 'last', 'updated', 'may', '20', '2012', 'language', 'english', 'start', 'of', 'this', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', '2002', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'by', 'david', 'wyllie', 'i', 'one', 'morning', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'he', 'lay', 'on', 'his', 'armour', 'like', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'the', 'bedding', 'was']\n"
     ]
    }
   ],
   "source": [
    "#normalizing_case\n",
    "#split into words by white space\n",
    "words = text.split()\n",
    "# or by regex\n",
    "words = re.split(r'\\W+', text)\n",
    "\n",
    "# convert to lower case\n",
    "words = [word.lower() for word in words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Cleaning with NLTK\n",
    "The Natural Language Toolkit, or NLTK for short, is a Python library written for working and\n",
    "modeling text. It provides good tools for loading and cleaning text that we can use to get our\n",
    "data ready for working with machine learning and deep learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Metamorphosis\n",
      "\n",
      "Author: Franz Kafka\n",
      "\n",
      "Translator: David Wyllie\n",
      "\n",
      "Release Date: August 16, 2005 [EBook #5200]\n",
      "First posted: May 13, 2002\n",
      "Last updated: May 20, 2012\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "*** START OF THIS PROJECT GUTENBERG EBOOK METAMORPHOSIS ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Copyright (C) 2002 David Wyllie.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/anshumanguha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', ':', 'Metamorphosis', 'Author', ':', 'Franz', 'Kafka', 'Translator', ':', 'David', 'Wyllie', 'Release', 'Date', ':', 'August', '16', ',', '2005', '[', 'EBook', '#', '5200', ']', 'First', 'posted', ':', 'May', '13', ',', '2002', 'Last', 'updated', ':', 'May', '20', ',', '2012', 'Language', ':', 'English', '***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', '***', 'Copyright', '(', 'C', ')', '2002', 'David', 'Wyllie', '.', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning', ',', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', ',', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', '.', 'He', 'lay', 'on', 'his', 'armour-like', 'back', ',', 'and', 'if', 'he', 'lifted', 'his']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title', 'Metamorphosis', 'Author', 'Franz', 'Kafka', 'Translator', 'David', 'Wyllie', 'Release', 'Date', 'August', 'EBook', 'First', 'posted', 'May', 'Last', 'updated', 'May', 'Language', 'English', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'METAMORPHOSIS', 'Copyright', 'C', 'David', 'Wyllie', 'Metamorphosis', 'Franz', 'Kafka', 'Translated', 'by', 'David', 'Wyllie', 'I', 'One', 'morning', 'when', 'Gregor', 'Samsa', 'woke', 'from', 'troubled', 'dreams', 'he', 'found', 'himself', 'transformed', 'in', 'his', 'bed', 'into', 'a', 'horrible', 'vermin', 'He', 'lay', 'on', 'his', 'back', 'and', 'if', 'he', 'lifted', 'his', 'head', 'a', 'little', 'he', 'could', 'see', 'his', 'brown', 'belly', 'slightly', 'domed', 'and', 'divided', 'by', 'arches', 'into', 'stiff', 'sections', 'The', 'bedding', 'was', 'hardly', 'able', 'to', 'cover', 'it', 'and', 'seemed', 'ready', 'to', 'slide']\n"
     ]
    }
   ],
   "source": [
    "# remove all tokens that are not alphabetic\n",
    "words = [word for word in tokens if word.isalpha()]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/anshumanguha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# filter stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'metamorphosis', 'author', 'franz', 'kafka', 'translator', 'david', 'wyllie', 'release', 'date', 'august', 'ebook', 'first', 'posted', 'may', 'last', 'updated', 'may', 'language', 'english', 'start', 'project', 'gutenberg', 'ebook', 'metamorphosis', 'copyright', 'c', 'david', 'wyllie', 'metamorphosis', 'franz', 'kafka', 'translated', 'david', 'wyllie', 'one', 'morning', 'gregor', 'samsa', 'woke', 'troubled', 'dreams', 'found', 'transformed', 'bed', 'horrible', 'vermin', 'lay', 'armourlike', 'back', 'lifted', 'head', 'little', 'could', 'see', 'brown', 'belly', 'slightly', 'domed', 'divided', 'arches', 'stiff', 'sections', 'bedding', 'hardly', 'able', 'cover', 'seemed', 'ready', 'slide', 'moment', 'many', 'legs', 'pitifully', 'thin', 'compared', 'size', 'rest', 'waved', 'helplessly', 'looked', 'happened', 'thought', 'nt', 'dream', 'room', 'proper', 'human', 'room', 'although', 'little', 'small', 'lay', 'peacefully', 'four', 'familiar', 'walls', 'collection', 'textile', 'samples']\n"
     ]
    }
   ],
   "source": [
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "# prepare regex for char filtering\n",
    "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "# remove punctuation from each word\n",
    "stripped = [re_punc.sub('', w) for w in tokens]\n",
    "# remove remaining tokens that are not alphabetic\n",
    "words = [word for word in stripped if word.isalpha()]\n",
    "# filter out stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [w for w in words if not w in stop_words]\n",
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stem Words\n",
    "Stemming refers to the process of reducing each word to its root or base. For example  shing,\n",
    " shed,  sher all reduce to the stem  sh. Some applications, like document classi cation, may\n",
    "bene t from stemming in order to both reduce the vocabulary and to focus on the sense or\n",
    "sentiment of a document rather than deeper meaning. There are many stemming algorithms,\n",
    "although a popular and long-standing method is the Porter Stemming algorithm. This method\n",
    "is available in NLTK via the PorterStemmer class. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['titl', ':', 'metamorphosi', 'author', ':', 'franz', 'kafka', 'translat', ':', 'david', 'wylli', 'releas', 'date', ':', 'august', '16', ',', '2005', '[', 'ebook', '#', '5200', ']', 'first', 'post', ':', 'may', '13', ',', '2002', 'last', 'updat', ':', 'may', '20', ',', '2012', 'languag', ':', 'english', '***', 'start', 'OF', 'thi', 'project', 'gutenberg', 'ebook', 'metamorphosi', '***', 'copyright', '(', 'C', ')', '2002', 'david', 'wylli', '.', 'metamorphosi', 'franz', 'kafka', 'translat', 'by', 'david', 'wylli', 'I', 'one', 'morn', ',', 'when', 'gregor', 'samsa', 'woke', 'from', 'troubl', 'dream', ',', 'he', 'found', 'himself', 'transform', 'in', 'hi', 'bed', 'into', 'a', 'horribl', 'vermin', '.', 'He', 'lay', 'on', 'hi', 'armour-lik', 'back', ',', 'and', 'if', 'he', 'lift', 'hi']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "# load data\n",
    "filename = 'metamorphosis_clean.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "file.close()\n",
    "# split into words\n",
    "tokens = word_tokenize(text)\n",
    "# stemming of words\n",
    "porter = PorterStemmer()\n",
    "stemmed = [porter.stem(word) for word in tokens]\n",
    "print(stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Text Cleaning Considerations\n",
    " Here is a shortlist of additional considerations when cleaning text:\n",
    "  \n",
    "  \n",
    "  Handling large documents and large collections of text documents that do not  t into\n",
    "memory.\n",
    "  \n",
    "  \n",
    "  Extracting text from markup like HTML, PDF, or other structured document formats.\n",
    "  \n",
    "  \n",
    "  Transliteration of characters from other languages into English.\n",
    "  \n",
    "  \n",
    "  Decoding Unicode characters into a normalized form, such as UTF8.\n",
    "  \n",
    "  \n",
    "  Handling of domain specific words, phrases, and acronyms.\n",
    "  \n",
    "  \n",
    "  Handling or removing numbers, such as dates and amounts.\n",
    "  \n",
    "  \n",
    "  Locating and correcting common typos and misspellings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Prepare Text Data with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model\n",
    "### Word Counts with CountVectorizer\n",
    "\n",
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents\n",
    "and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n",
    "You can use it as follows:\n",
    " Create an instance of the CountVectorizer class.\n",
    " Call the fit() function in order to learn a vocabulary from one or more documents.\n",
    " Call the transform() function on one or more documents as needed to encode each as a\n",
    "vector.\n",
    "An encoded vector is returned with a length of the entire vocabulary and an integer count\n",
    "for the number of times each word appeared in the document. Because these vectors will\n",
    "contain a lot of zeros, we call them sparse. Python provides an e\u000ecient way of handling sparse\n",
    "vectors in the scipy.sparse package. The vectors returned from a call to transform() will\n",
    "be sparse vectors, and you can transform them back to NumPy arrays to look and better\n",
    "understand what is going on by calling the toarray() function. Below is an example of using\n",
    "the CountVectorizer to tokenize, build a vocabulary, and then encode a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(vectorizer.vocabulary_)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# encode another document\n",
    "text2 = [\"the puppy\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequencies with TfidfVectorizer\n",
    "\n",
    "Word counts are a good starting point, but are very basic. One issue with simple counts is that\n",
    "some words like the will appear many times and their large counts will not be very meaningful\n",
    "in the encoded vectors. An alternative is to calculate word frequencies, and by far the most\n",
    "popular method is called TF-IDF. This is an acronym that stands for Term Frequency - Inverse\n",
    "Document Frequency which are the components of the resulting scores assigned to each word.\n",
    " Term Frequency: This summarizes how often a given word appears within a document.\n",
    " Inverse Document Frequency: This downscales words that appear a lot across docu-\n",
    "ments.\n",
    "\n",
    "Without going into the math, TF-IDF are word frequency scores that try to highlight\n",
    "words that are more interesting, e.g. frequent in a document but not across documents.\n",
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a\n",
    "learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse\n",
    "document frequencies and start encoding documents. The same create, \f",
    "t, and transform process\n",
    "is used as with the CountVectorizer. Below is an example of using the TfidfVectorizer to\n",
    "learn vocabulary and inverse document frequencies across 3 small documents and then encode\n",
    "one of those documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer.vocabulary_ {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "vectorizer.idf_ [1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n",
      " 1.69314718 1.        ]\n",
      "(1, 8)\n",
      "[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "\"The dog.\",\n",
    "\"The fox\"]\n",
    "# create the transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "# summarize\n",
    "print(\"vectorizer.vocabulary_\", vectorizer.vocabulary_)\n",
    "print(\"vectorizer.idf_\", vectorizer.idf_)\n",
    "# encode document\n",
    "vector = vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing with HashingVectorizer\n",
    "\n",
    "Counts and frequencies can be very useful, but one limitation of these methods is that the\n",
    "vocabulary can become very large. This, in turn, will require large vectors for encoding\n",
    "documents and impose large requirements on memory and slow down algorithms. A clever work\n",
    "around is to use a one way hash of words to convert them to integers. The clever part is that\n",
    "no vocabulary is required and you can choose an arbitrary-long \f",
    "xed length vector. A downside\n",
    "is that the hash is a one-way function so there is no way to convert the encoding back to a word\n",
    "(which may not matter for many supervised learning tasks).\n",
    "The HashingVectorizer class implements this approach that can be used to consistently\n",
    "hash words, then tokenize and encode documents as needed. The example below demonstrates\n",
    "the HashingVectorizer for encoding a single document. An arbitrary \f",
    "xed-length vector size\n",
    "of 20 was chosen. This corresponds to the range of the hash function, where small values (like\n",
    "20) may result in hash collisions. Remembering back to Computer Science classes, I believe\n",
    "there are heuristics that you can use to pick the hash length and probability of collision based\n",
    "on estimated vocabulary size (e.g. a load factor of 75%). See any good textbook on the topic.\n",
    "Note that this vectorizer does not require a call to \f",
    "t on the training data documents. Instead,\n",
    "after instantiation, it can be used directly to start encoding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Prepare Text Data With Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Words with text to word sequence\n",
    "\n",
    "A good \f",
    "rst step when working with text is to split it into words. Words are called to-\n",
    "kens and the process of splitting text into tokens is called tokenization. Keras provides the\n",
    "text to word sequence() function that you can use to split text into a list of words. By\n",
    "default, this function automatically does 3 things:\n",
    " Splits words by space.\n",
    " Filters out punctuation.\n",
    " Converts text to lowercase (lower=True).\n",
    "You can change any of these defaults by passing arguments to the function. Below is an\n",
    "example of using the text to word sequence() function to split a document (in this case a\n",
    "simple string) into a list of words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# tokenize the document\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding with one hot\n",
    "\n",
    "It is popular to represent a document as a sequence of integer values, where each word in the\n",
    "document is represented as a unique integer. Keras provides the one hot() function that you\n",
    "can use to tokenize and integer encode a text document in one step. The name suggests that it\n",
    "will create a one hot encoding of the document, which is not the case. Instead, the function\n",
    "is a wrapper for the hashing trick() function described in the next section. The function\n",
    "returns an integer encoded version of the document. The use of a hash function means that\n",
    "there may be collisions and not all words will be assigned unique integer values. As with the\n",
    "text to word sequence() function in the previous section, the one hot() function will make\n",
    "the text lower case, alter out punctuation, and split words based on white space.\n",
    "In addition to the text, the vocabulary size (total words) must be speci\f",
    "ed. This could be the\n",
    "total number of words in the document or more if you intend to encode additional documents\n",
    "that contains additional words. The size of the vocabulary de\f",
    "nes the hashing space from which\n",
    "words are hashed. By default, the hash function is used, although as we will see in the next\n",
    "section, alternate hash functions can be speci\f",
    "ed when calling the hashing trick() function\n",
    "directly.\n",
    "We can use the text to word sequence() function from the previous section to split the\n",
    "document into words and then use a set to represent only the unique words in the document.\n",
    "The size of this set can be used to estimate the size of the vocabulary for one document. For\n",
    "exampl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[2, 5, 3, 8, 4, 9, 2, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = one_hot(text, round(vocab_size*1.3))\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Encoding with hashing trick\n",
    "A limitation of integer and count base encodings is that they must maintain a vocabulary of\n",
    "words and their mapping to integers. An alternative to this approach is to use a one-way hash\n",
    "function to convert words to integers. This avoids the need to keep track of a vocabulary, which\n",
    "is faster and requires less memory.\n",
    "Keras provides the hashing trick() function that tokenizes and then integer encodes the\n",
    "document, just like the one hot() function. It provides more \n",
    "exibility, allowing you to specify\n",
    "the hash function as either hash (the default) or other hash functions such as the built in md5\n",
    "function or your own function. Below is an example of integer encoding a document using the\n",
    "md5 hash function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[6, 4, 1, 2, 7, 5, 6, 2, 6]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import hashing_trick\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "# define the document\n",
    "text = 'The quick brown fox jumped over the lazy dog.'\n",
    "# estimate the size of the vocabulary\n",
    "words = set(text_to_word_sequence(text))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)\n",
    "# integer encode the document\n",
    "result = hashing_trick(text, round(vocab_size*1.3), hash_function='md5')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer API\n",
    "\n",
    "So far we have looked at one-o\u000b",
    " convenience methods for preparing text with Keras. Keras\n",
    "provides a more sophisticated API for preparing text that can be \f",
    "t and reused to prepare\n",
    "multiple text documents. This may be the preferred approach for large projects. Keras provides\n",
    "the Tokenizer class for preparing text documents for deep learning. The Tokenizer must be\n",
    "constructed and then \f",
    "t on either raw text documents or integer encoded text documents. For\n",
    "example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once fit, the Tokenizer provides 4 attributes that you can use to query what has been\n",
    "learned about your documents:\n",
    "\n",
    " word counts: A dictionary mapping of words and their occurrence counts when the\n",
    "Tokenizer was fit.\n",
    "\n",
    " word docs: A dictionary mapping of words and the number of documents that reach\n",
    "appears in.\n",
    "\n",
    " word index: A dictionary of words and their uniquely assigned integers.\n",
    "\n",
    " document count: A dictionary mapping and the number of documents they appear in\n",
    "calculated during the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the Tokenizer has been fit on training data, it can be used to encode documents in\n",
    "the train or test datasets. The texts to matrix() function on the Tokenizer can be used to\n",
    "create one vector per document provided per input. The length of the vectors is the total size\n",
    "of the vocabulary. This function provides a suite of standard bag-of-words model text encoding\n",
    "schemes that can be provided via a mode argument to the function. The modes available\n",
    "include:\n",
    "\n",
    " binary: Whether or not each word is present in the document. This is the default.\n",
    "\n",
    " count: The count of each word in the document.\n",
    "\n",
    " tfidf: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word in the document.\n",
    "\n",
    " freq: The frequency of each word as a ratio of words within each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('well', 1), ('done', 1), ('good', 1), ('work', 2), ('great', 1), ('effort', 1), ('nice', 1), ('excellent', 1)])\n",
      "5\n",
      "{'work': 1, 'well': 2, 'done': 3, 'good': 4, 'great': 5, 'effort': 6, 'nice': 7, 'excellent': 8}\n",
      "{'done': 1, 'well': 1, 'work': 2, 'good': 1, 'great': 1, 'effort': 1, 'nice': 1, 'excellent': 1}\n",
      "[[0. 0. 1. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "# define 5 documents\n",
    "docs = ['Well done!',\n",
    "'Good work',\n",
    "'Great effort',\n",
    "'nice work',\n",
    "'Excellent!']\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(docs)\n",
    "# summarize what was learned\n",
    "print(t.word_counts)\n",
    "print(t.document_count)\n",
    "print(t.word_index)\n",
    "print(t.word_docs)\n",
    "# integer encode documents\n",
    "encoded_docs = t.texts_to_matrix(docs, mode='count')\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
