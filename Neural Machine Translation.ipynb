{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Translation?\n",
    "Machine translation is challenging given the inherent ambiguity and \n",
    "exibility of human language.\n",
    "\n",
    " Statistical machine translation replaces classical rule-based systems with models that learn\n",
    "to translate from examples.\n",
    "\n",
    " Neural machine translation models fit a single model rather than a pipeline of fine-tuned\n",
    "models and currently achieve state-of-the-art results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Translation?\n",
    "\n",
    "Machine translation is the task of automatically converting source text in one language to text\n",
    "in another language.\n",
    "\n",
    "Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and \n",
    "exibility of human language. This makes the challenge of automatic machine translation difficult, perhaps one of the most difficult in artificial intelligence:\n",
    "\n",
    "Classical machine translation methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate atthe lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Statistical Machine Translation?\n",
    "\n",
    "Statistical machine translation, or SMT for short, is the use of statistical models that learn to\n",
    "translate text from a source language to a target language given a large corpus of examples.\n",
    "\n",
    "\n",
    "This task of using a statistical model can be stated formally as follows:\n",
    "Given a sentence T in the target language, we seek the sentence S from which the\n",
    "translator produced T. We know that our chance of error is minimized by choosing\n",
    "that sentence S that is most probable given T. Thus, we wish to choose S so as to\n",
    "maximize Pr(S|T)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Neural Machine Translation?\n",
    "\n",
    "\n",
    "Neural machine translation, or NMT for short, is the use of neural network models to learn\n",
    "a statistical model for machine translation. The key benefitt to the approach is that a singlesystem can be trained directly on source and target text, no longer requiring the pipeline of specialized systems used in statistical machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Model\n",
    "\n",
    "Multilayer Perceptron neural network models can be used for machine translation, although the\n",
    "models are limited by a fixed-length input sequence where the output must be the same length.\n",
    "These early models have been greatly improved upon recently through the use of recurrent\n",
    "neural networks organized into an encoder-decoder architecture that allow for variable length\n",
    "input and output sequences.\n",
    "\n",
    "\n",
    "An encoder neural network reads and encodes a source sentence into a fixed-length\n",
    "vector. A decoder then outputs a translation from the encoded vector. The whole\n",
    "encoder-decoder system, which consists of the encoder and the decoder for a language\n",
    "pair, is jointly trained to maximize the probability of a correct translation given a\n",
    "source sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoders with Attention\n",
    "\n",
    "Although effective, the Encoder-Decoder architecture has problems with long sequences of text to be translated. The problem stems from the fixed-length internal representation that must be used to decode each word in the output sequence. The solution is the use of an attention mechanism that allows the model to learn where to place attention on the input sequence as each word of the output sequence is decoded.\n",
    "\n",
    "\n",
    "Using a fixed-sized representation to capture all the semantic details of a very long\n",
    "sentence [...] is very difficult. [...] A more efficient approach, however, is to read\n",
    "the whole sentence or paragraph [...], then to produce the translated words one at\n",
    "a time, each time focusing on a different part of the input sentence to gather the\n",
    "semantic details required to produce the next output word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Encoder-Decoder Models for Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Architecture for NMT\n",
    "\n",
    "The Encoder-Decoder architecture with recurrent neural networks has become an effective\n",
    "and standard approach for both neural machine translation (NMT) and sequence-to-sequence\n",
    "(seq2seq) prediction in general. The key benefits of the approach are the ability to train a single\n",
    "end-to-end model directly on source and target sentences and the ability to handle variable\n",
    "length input and output sequences of text. As evidence of the success of the method, the\n",
    "architecture is the core of the Google translation service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sutskever NMT Model\n",
    "\n",
    "In this section, we will look at the neural machine translation model developed by Ilya Sutskever,\n",
    "et al. as described in their 2014 paper Sequence to Sequence Learning with Neural Networks.\n",
    "We will refer to it as the Sutskever NMT Model, for lack of a better name. This is an important\n",
    "paper as it was one of the first to introduce the Encoder-Decoder model for machine translation\n",
    "and more generally sequence-to-sequence learning. It is an important model in the field of\n",
    "machine translation as it was one of the first neural machine translation systems to outperform\n",
    "a baseline statistical machine learning model on a large translation task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "The model was applied to English to French translation, specifically the WMT 2014 translation\n",
    "task. The translation task was processed one sentence at a time, and an end-of-sequence (<EOS>)\n",
    "token was added to the end of output sequences during training to signify the end of the\n",
    "translated sequence. This allowed the model to be capable of predicting variable length output\n",
    "sequences.\n",
    "\n",
    "\n",
    "    Note that we require that each sentence ends with a special end-of-sentence symbol <EOS>, which enables the model to dense a distribution over sequences of all possible lengths.\n",
    "\n",
    "\n",
    "    | Sequence to Sequence Learning with Neural Networks, 2014.\n",
    "\n",
    "\n",
    "The model was trained on a subset of the 12 Million sentences in the dataset, comprised of\n",
    "348 Million French words and 304 Million English words. This set was chosen because it was\n",
    "pre-tokenized. The source vocabulary was reduced to the 160,000 most frequent source English\n",
    "words and 80,000 of the most frequent target French words. All out-of-vocabulary words were\n",
    "replaced with the UNK token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "\n",
    "An Encoder-Decoder architecture was developed where an input sequence was read in entirety and encoded to a fixed-length internal representation. A decoder network then used this internal\n",
    "representation to output words until the end of sequence token was reached. LSTM networks were used for both the encoder and decoder.\n",
    "\n",
    "\n",
    "The idea is to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "The following provides a summary of the model configuration taken from the paper:\n",
    "    \n",
    " Input sequences were reversed.\n",
    "\n",
    " A 1000-dimensional word embedding layer was used to represent the input words.\n",
    "\n",
    " Softmax was used on the output layer.\n",
    "\n",
    " The input and output models had 4 layers with 1,000 units per layer.\n",
    "\n",
    " The model was fit for 7.5 epochs where some learning rate decay was performed.\n",
    "\n",
    " A batch-size of 128 sequences was used during training.\n",
    "\n",
    " Gradient clipping was used during training to mitigate the chance of gradient explosions.\n",
    "\n",
    " Batches were comprised of sentences with roughly the same length to speed-up computation.\n",
    "\n",
    "The model was fit on an 8-GPU machine where each layer was run on a different GPU.\n",
    "Training took 10 days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cho NMT Model\n",
    "\n",
    "Neural machine translation system described by Kyunghyun Cho, et al. in their 2014 paper titled Learning Phrase Representations using RNN Encoder-\n",
    "Decoder for Statistical Machine Translation. \n",
    "\n",
    "\n",
    "Importantly, the Cho Model is used only to score candidate translations and is not used directly for translation like the Sutskever model above. Although extensions to\n",
    "the work to better diagnose and improve the model do use it directly and alone for translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "As above, the problem is the English to French translation task from the WMT 2014 workshop.\n",
    "The source and target vocabulary were limited to the most frequent 15,000 French and English\n",
    "words which covers 93% of the dataset, and out of vocabulary words were replaced with UNK.\n",
    "... called RNN Encoder-Decoder that consists of two recurrent neural networks\n",
    "(RNN). One RNN encodes a sequence of symbols into a fixed-length vector rep-\n",
    "resentation, and the other decodes the representation into another sequence of\n",
    "symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "In the paper On the Properties of Neural Machine Translation: Encoder-Decoder Approaches,\n",
    "Cho, et al. investigate the limitations of their model. They discover that performance degrades\n",
    "quickly with the increase in the length of input sentences and with the number of words outside\n",
    "of the vocabulary.\n",
    "\n",
    "Our analysis revealed that the performance of the neural machine translation su\u000b",
    "ers\n",
    "significantly from the length of sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the problem of unknown words, they suggest dramatically increasing the vocabu-\n",
    "lary of known words during training. They address the problem of sentence length in a follow-up\n",
    "paper titled Neural Machine Translation by Jointly Learning to Align and Translate in which\n",
    "they propose the use of an attention mechanism. Instead of encoding the input sentence to a\n",
    "fixed length vector, a fuller representation of the encoded input is kept and the model learns to\n",
    "use to pay attention to di\u000b",
    "erent parts of the input for each word output by the decoder.\n",
    "Each time the proposed model generates a word in a translation, it (soft-)searches\n",
    "for a set of positions in a source sentence where the most relevant information is\n",
    "concentrated. The model then predicts a target word based on the context vectors\n",
    "associated with these source positions and all the previous generated target words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A wealth of technical details are provided in the paper; for example:\n",
    "\n",
    " A similarly configured model is used, although with bidirectional layers.\n",
    "\n",
    "\n",
    " The data is prepared such that 30,000 of the most common words are kept in the vocabulary.\n",
    "\n",
    " The model is first trained with sentences with a length up to 20 words, then with sentences\n",
    "with a length up to 50 words.\n",
    "\n",
    " A batch size of 80 sentences is used and the model was fit for 4-6 epochs.\n",
    "\n",
    " A beam search was used during the inference to \f",
    "nd the most likely sequence of words for\n",
    "each translation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Litrerature review of:\n",
    "## https://arxiv.org/abs/1703.03906\n",
    "\n",
    "\"Massive Exploration of Neural Machine Translation Architectures\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Configure Encoder-Decoder Models for Machine Translation\n",
    "\n",
    "The encoder-decoder architecture for recurrent neural networks is achieving state-of-the-art\n",
    "results on standard machine translation benchmarks and is being used in the heart of industrial\n",
    "translation services. The model is simple, but given the large amount of data required to train it,\n",
    "tuning the myriad of design decisions in the model in order get top performance on your problem\n",
    "can be practically intractable. Thankfully, research scientists have used Google-scale hardware\n",
    "to do this work for us and provide a set of heuristics for how to con\f",
    "gure the encoder-decoder\n",
    "model for neural machine translation and for sequence prediction generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Model for Neural Machine Translation\n",
    "\n",
    "The Encoder-Decoder architecture for recurrent neural networks is displacing classical phrase-\n",
    "based statistical machine translation systems for state-of-the-art results. As evidence, by their\n",
    "2016 paper Google's Neural Machine Translation System: Bridging the Gap between Human\n",
    "and Machine Translation, Google now uses the approach in their core of their Google Translate\n",
    "service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "  Embedding: 512-dimensions.\n",
    "  \n",
    " RNN Cell: Gated Recurrent Unit or GRU.\n",
    "\n",
    " Encoder: Bidirectional.\n",
    "\n",
    " Encoder Depth: 2-layers (1 layer in each direction).\n",
    "\n",
    " Decoder Depth: 2-layers.\n",
    "\n",
    " Attention: Bahdanau-style.\n",
    "\n",
    " Optimizer: Adam.\n",
    "\n",
    " Dropout: 20% on input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding Size\n",
    "\n",
    "A word-embedding is used to represent words input to the encoder. This is a distributed\n",
    "representation where each word is mapped to a \f",
    "xed-sized vector of continuous values. The\n",
    "benefit of this approach is that di\u000b",
    "erent words with similar meaning will have a similar\n",
    "representation. This distributed representation is often learned while \f",
    "tting the model on the\n",
    "training data. The embedding size de\f",
    "nes the length of the vectors used to represent words. It\n",
    "is generally believed that a larger dimensionality will result in a more expressive representation,\n",
    "and in turn, better skill. Interestingly, the results show that the largest size tested did achieve\n",
    "the best results, but the bene\f",
    "t of increasing the size was minor overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Cell Type\n",
    "\n",
    "There are generally three types of recurrent neural network cells that are commonly used:\n",
    " Simple RNN.\n",
    "\n",
    " Long Short-Term Memory or LSTM.\n",
    "\n",
    " Gated Recurrent Unit or GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The LSTM was developed to address the vanishing gradient problem of the Simple RNN**\n",
    "that limited the training of deep RNNs. \n",
    "\n",
    "The GRU was developed in an attempt to simplify the LSTM. Results showed that both the GRU and LSTM were significantly better than the Simple\n",
    "RNN, but the LSTM was generally better overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Depth\n",
    "\n",
    "\n",
    "Generally, deeper networks are believed to achieve better performance than shallow networks.\n",
    "The key is to find a balance between network depth, model skill, and training time. \n",
    "\n",
    "This is because we generally do not have infinite resources to train very deep networks if the benefit\n",
    "to skill is minor. The authors explore the depth of both the encoder and decoder models and\n",
    "the impact on model skill. \n",
    "\n",
    "When it comes to encoders, it was found that depth did not have a dramatic impact on skill and more surprisingly, a 1-layer unidirectional model performs only\n",
    "slightly worse than a 4-layer unidirectional configuration. \n",
    "\n",
    "\n",
    "A two-layer bidirectional encoder performed slightly better than other configurations tested.\n",
    "\n",
    "\n",
    "\n",
    "**Recommendation**: Use a 1-layer bidirectional encoder and extend to 2 bidirectional layers\n",
    "for a small lift in skill.\n",
    "A similar story was seen when it came to decoders. The skill between decoders with 1, 2,\n",
    "and 4 layers was different by a small amount where a 4-layer decoder was slightly better. An\n",
    "8-layer decoder did not converge under the test conditions.\n",
    "\n",
    "On the decoder side, deeper models outperformed shallower ones by a small margin.\n",
    "| Massive Exploration of Neural Machine Translation Architectures, 2017.\n",
    "\n",
    "\n",
    "**Recommendation**: Use a 1-layer decoder as a starting point and use a 4-layer decoder for\n",
    "better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direction of Encoder Input\n",
    "\n",
    "The order of the sequence of source text can be provided to the encoder a number of ways:\n",
    " Forward or as-normal.\n",
    " Reversed.\n",
    " Both forward and reversed at the same time.\n",
    "\n",
    "\n",
    "\n",
    "The authors explored the impact of the order of the input sequence on model skill comparing\n",
    "various unidirectional and bidirectional configurations. Generally, they confirmed previous\n",
    "findings that a reversed sequence is better than a forward sequence and that bidirectional is\n",
    "slightly better than a reversed sequence.\n",
    "... bidirectional encoders generally outperform unidirectional encoders, but not by\n",
    "a large margin. The encoders with reversed source consistently outperform their\n",
    "non-reversed counterparts.\n",
    " $- Massive Exploration of Neural Machine Translation Architectures, 2017.$\n",
    "\n",
    "\n",
    "**Recommendation**: Use a reversed order input sequence or move to bidirectional for a\n",
    "small lift in model skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Mechanism\n",
    "\n",
    "A problem with the naive Encoder-Decoder model is that the encoder maps the input to a\n",
    "fixed-length internal representation from which the decoder must produce the entire output\n",
    "sequence. Attention is an improvement to the model that allows the decoder to pay attention to different words in the input sequence as it outputs each word in the output sequence. The authors look at a few variations on simple attention mechanisms. The results show that having attention results in dramatically better performance than not having attention.\n",
    "\n",
    "\n",
    "While we did expect the attention-based models to signi\f",
    "cantly outperform those without an attention mechanism, we were surprised by just how poorly the [no attention] models fared.\n",
    "\n",
    " $- Massive Exploration of Neural Machine Translation Architectures, 2017.$\n",
    "\n",
    "\n",
    "The simple weighted average style attention described by Bahdanau, et al. in their 2015\n",
    "paper Neural machine translation by jointly learning to align and translate was found to perform the best.\n",
    "\n",
    "**Recommendation**: Use attention and prefer the Bahdanau-style weighted average style\n",
    "attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "It is common in neural machine translation systems to use a beam-search to sample the\n",
    "probabilities for the words in the sequence output by the model. The wider the beam width, the\n",
    "more exhaustive the search, and, it is believed, the better the results. The results showed that\n",
    "a modest beam-width of 3-5 performed the best, which could be improved only very slightly\n",
    "through the use of length penalties. The authors generally recommend tuning the beam width\n",
    "on each specific problem.\n",
    "\n",
    "We found that a well-tuned beam search is crucial to achieving good results, and\n",
    "that it leads to consistent gains of more than one BLEU point\n",
    " \n",
    " $- Massive Exploration of Neural Machine Translation Architectures, 2017.$\n",
    "\n",
    "    \n",
    "**Recommendation**: Start with a greedy search (beam=1) and tune based on your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "The authors pull together their findings into a single best model and compare the results of this model to other well-performing models and state-of-the-art results. The specific configurations of this model are summarized in the table below, taken from the paper. These parameters may be taken as a good or best starting point when developing your own encoder-decoder model for an NLP application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Develop a Neural Machine Translation Model\n",
    "\n",
    "Tutorial Overview\n",
    "This tutorial is divided into the following parts:\n",
    "1. German to English Translation Dataset\n",
    "2. Preparing the Text Data\n",
    "3. Train Neural Translation Model\n",
    "4. Evaluate Neural Translation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the English-German pairs dataset.\n",
    "http://www.manythings.org/anki/deu-eng.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german.pkl\n",
      "[go] => [geh]\n",
      "[hi] => [hallo]\n",
      "[hi] => [gru gott]\n",
      "[run] => [lauf]\n",
      "[run] => [lauf]\n",
      "[wow] => [potzdonner]\n",
      "[wow] => [donnerwetter]\n",
      "[duck] => [kopf runter]\n",
      "[fire] => [feuer]\n",
      "[help] => [hilfe]\n",
      "[help] => [zu hulf]\n",
      "[stay] => [bleib]\n",
      "[stop] => [stopp]\n",
      "[stop] => [anhalten]\n",
      "[wait] => [warte]\n",
      "[wait] => [warte]\n",
      "[begin] => [fang an]\n",
      "[do it] => [mache es]\n",
      "[do it] => [tue es]\n",
      "[go on] => [mach weiter]\n",
      "[hello] => [hallo]\n",
      "[hello] => [sers]\n",
      "[hurry] => [beeil dich]\n",
      "[hurry] => [schnell]\n",
      "[i hid] => [ich versteckte mich]\n",
      "[i hid] => [ich habe mich versteckt]\n",
      "[i ran] => [ich rannte]\n",
      "[i see] => [ich verstehe]\n",
      "[i see] => [aha]\n",
      "[i try] => [ich versuche es]\n",
      "[i try] => [ich probiere es]\n",
      "[i won] => [ich hab gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[i won] => [ich habe gewonnen]\n",
      "[oh no] => [oh nein]\n",
      "[relax] => [entspann dich]\n",
      "[shoot] => [feuer]\n",
      "[shoot] => [schie]\n",
      "[smile] => [lacheln]\n",
      "[sorry] => [entschuldigung]\n",
      "[ask me] => [frag mich]\n",
      "[ask me] => [fragt mich]\n",
      "[ask me] => [fragen sie mich]\n",
      "[attack] => [angriff]\n",
      "[attack] => [attacke]\n",
      "[buy it] => [kaufs]\n",
      "[cheers] => [zum wohl]\n",
      "[eat it] => [iss es]\n",
      "[eat up] => [iss auf]\n",
      "[eat up] => [iss auf]\n",
      "[freeze] => [keine bewegung]\n",
      "[freeze] => [stehenbleiben]\n",
      "[go now] => [geh jetzt]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [ich habs]\n",
      "[got it] => [aha]\n",
      "[got it] => [kapiert]\n",
      "[got it] => [verstanden]\n",
      "[got it] => [einverstanden]\n",
      "[he ran] => [er rannte]\n",
      "[he ran] => [er lief]\n",
      "[hop in] => [mach mit]\n",
      "[hop in] => [spring rein]\n",
      "[hug me] => [druck mich]\n",
      "[hug me] => [nimm mich in den arm]\n",
      "[hug me] => [umarme mich]\n",
      "[i care] => [mir ist es wichtig]\n",
      "[i fell] => [ich fiel]\n",
      "[i fell] => [ich fiel hin]\n",
      "[i fell] => [ich sturzte]\n",
      "[i fell] => [ich bin hingefallen]\n",
      "[i fell] => [ich bin gesturzt]\n",
      "[i fled] => [ich fluchtete]\n",
      "[i fled] => [ich bin gefluchtet]\n",
      "[i know] => [ich wei]\n",
      "[i lied] => [ich habe gelogen]\n",
      "[i lost] => [ich habe verloren]\n",
      "[i paid] => [ich habe bezahlt]\n",
      "[i paid] => [ich zahlte]\n",
      "[i sang] => [ich sang]\n",
      "[i spit] => [ich spuckte]\n",
      "[i spit] => [ich habe gespuckt]\n",
      "[i swim] => [ich schwimme]\n",
      "[i wept] => [ich weinte]\n",
      "[i wept] => [ich habe geweint]\n",
      "[im] => [ich bin jahre alt]\n",
      "[im] => [ich bin]\n",
      "[im ok] => [mir gehts gut]\n",
      "[im ok] => [es geht mir gut]\n",
      "[im up] => [ich bin wach]\n",
      "[im up] => [ich bin auf]\n",
      "[listen] => [hort zu]\n",
      "[no way] => [unmoglich]\n",
      "[no way] => [das kommt nicht in frage]\n",
      "[no way] => [das gibts doch nicht]\n",
      "[no way] => [ausgeschlossen]\n",
      "[no way] => [in keinster weise]\n",
      "[really] => [wirklich]\n",
      "[really] => [echt]\n",
      "[really] => [im ernst]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in lines]\n",
    "    return pairs\n",
    "\n",
    "\n",
    "# clean a list of lines\n",
    "def clean_pairs(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [re_punc.sub('', w) for w in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return array(cleaned)\n",
    "\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'), protocol=4)\n",
    "    print('Saved: %s' % filename)\n",
    "# load dataset\n",
    "filename = './deu-eng/deu.txt'\n",
    "doc = load_doc(filename)\n",
    "# split into english-german pairs\n",
    "pairs = to_pairs(doc)\n",
    "# clean sentences\n",
    "clean_pairs = clean_pairs(pairs)\n",
    "# save clean pairs to file\n",
    "save_clean_data(clean_pairs, 'english-german.pkl')\n",
    "# spot check\n",
    "for i in range(100):\n",
    "    print('[%s] => [%s]' % (clean_pairs[i,0], clean_pairs[i,1]))\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: english-german-both.pkl\n",
      "Saved: english-german-train.pkl\n",
      "Saved: english-german-test.pkl\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from pickle import dump\n",
    "from numpy.random import shuffle\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)\n",
    "     \n",
    "    \n",
    "# load dataset\n",
    "raw_dataset = load_clean_sentences('english-german.pkl')\n",
    "# reduce dataset size\n",
    "n_sentences = 10000\n",
    "dataset = raw_dataset[:n_sentences, :]\n",
    "# random shuffle\n",
    "shuffle(dataset)\n",
    "# split into train/test\n",
    "train, test = dataset[:9000], dataset[9000:]\n",
    "\n",
    "# save\n",
    "save_clean_data(dataset, 'english-german-both.pkl')\n",
    "save_clean_data(train, 'english-german-train.pkl')\n",
    "save_clean_data(test, 'english-german-test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Neural Translation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 2185\n",
      "English Max Length: 5\n",
      "German Vocabulary Size: 3529\n",
      "German Max Length: 9\n",
      "WARNING:tensorflow:From /Users/anshumanguha/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 9, 256)            903424    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 5, 256)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 256)            525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 5, 2185)           561545    \n",
      "=================================================================\n",
      "Total params: 2,515,593\n",
      "Trainable params: 2,515,593\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /Users/anshumanguha/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      " - 13s - loss: 4.1305 - val_loss: 3.3669\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 3.36687, saving model to model.h5\n",
      "Epoch 2/30\n",
      " - 11s - loss: 3.2107 - val_loss: 3.1949\n",
      "\n",
      "Epoch 00002: val_loss improved from 3.36687 to 3.19492, saving model to model.h5\n",
      "Epoch 3/30\n",
      " - 12s - loss: 3.0670 - val_loss: 3.1183\n",
      "\n",
      "Epoch 00003: val_loss improved from 3.19492 to 3.11833, saving model to model.h5\n",
      "Epoch 4/30\n",
      " - 13s - loss: 2.9337 - val_loss: 2.9982\n",
      "\n",
      "Epoch 00004: val_loss improved from 3.11833 to 2.99818, saving model to model.h5\n",
      "Epoch 5/30\n",
      " - 13s - loss: 2.7910 - val_loss: 2.8904\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.99818 to 2.89044, saving model to model.h5\n",
      "Epoch 6/30\n",
      " - 13s - loss: 2.6349 - val_loss: 2.7885\n",
      "\n",
      "Epoch 00006: val_loss improved from 2.89044 to 2.78847, saving model to model.h5\n",
      "Epoch 7/30\n",
      " - 13s - loss: 2.4680 - val_loss: 2.6394\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.78847 to 2.63936, saving model to model.h5\n",
      "Epoch 8/30\n",
      " - 13s - loss: 2.3129 - val_loss: 2.5446\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.63936 to 2.54459, saving model to model.h5\n",
      "Epoch 9/30\n",
      " - 13s - loss: 2.1695 - val_loss: 2.4472\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.54459 to 2.44718, saving model to model.h5\n",
      "Epoch 10/30\n",
      " - 12s - loss: 2.0274 - val_loss: 2.3556\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.44718 to 2.35559, saving model to model.h5\n",
      "Epoch 11/30\n",
      " - 12s - loss: 1.8887 - val_loss: 2.2767\n",
      "\n",
      "Epoch 00011: val_loss improved from 2.35559 to 2.27669, saving model to model.h5\n",
      "Epoch 12/30\n",
      " - 12s - loss: 1.7634 - val_loss: 2.2055\n",
      "\n",
      "Epoch 00012: val_loss improved from 2.27669 to 2.20547, saving model to model.h5\n",
      "Epoch 13/30\n",
      " - 12s - loss: 1.6395 - val_loss: 2.1402\n",
      "\n",
      "Epoch 00013: val_loss improved from 2.20547 to 2.14015, saving model to model.h5\n",
      "Epoch 14/30\n",
      " - 13s - loss: 1.5243 - val_loss: 2.0748\n",
      "\n",
      "Epoch 00014: val_loss improved from 2.14015 to 2.07477, saving model to model.h5\n",
      "Epoch 15/30\n",
      " - 13s - loss: 1.4140 - val_loss: 2.0236\n",
      "\n",
      "Epoch 00015: val_loss improved from 2.07477 to 2.02359, saving model to model.h5\n",
      "Epoch 16/30\n",
      " - 14s - loss: 1.3069 - val_loss: 1.9926\n",
      "\n",
      "Epoch 00016: val_loss improved from 2.02359 to 1.99261, saving model to model.h5\n",
      "Epoch 17/30\n",
      " - 14s - loss: 1.2071 - val_loss: 1.9351\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.99261 to 1.93513, saving model to model.h5\n",
      "Epoch 18/30\n",
      " - 16s - loss: 1.1107 - val_loss: 1.8893\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.93513 to 1.88931, saving model to model.h5\n",
      "Epoch 19/30\n",
      " - 17s - loss: 1.0174 - val_loss: 1.8661\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.88931 to 1.86609, saving model to model.h5\n",
      "Epoch 20/30\n",
      " - 17s - loss: 0.9344 - val_loss: 1.8265\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.86609 to 1.82651, saving model to model.h5\n",
      "Epoch 21/30\n",
      " - 18s - loss: 0.8494 - val_loss: 1.7941\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.82651 to 1.79410, saving model to model.h5\n",
      "Epoch 22/30\n",
      " - 17s - loss: 0.7767 - val_loss: 1.7784\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.79410 to 1.77836, saving model to model.h5\n",
      "Epoch 23/30\n",
      " - 18s - loss: 0.7031 - val_loss: 1.7600\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.77836 to 1.76002, saving model to model.h5\n",
      "Epoch 24/30\n",
      " - 16s - loss: 0.6398 - val_loss: 1.7400\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.76002 to 1.74002, saving model to model.h5\n",
      "Epoch 25/30\n",
      " - 16s - loss: 0.5824 - val_loss: 1.7295\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.74002 to 1.72951, saving model to model.h5\n",
      "Epoch 26/30\n",
      " - 20s - loss: 0.5291 - val_loss: 1.7252\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.72951 to 1.72523, saving model to model.h5\n",
      "Epoch 27/30\n",
      " - 15s - loss: 0.4818 - val_loss: 1.7103\n",
      "\n",
      "Epoch 00027: val_loss improved from 1.72523 to 1.71031, saving model to model.h5\n",
      "Epoch 28/30\n",
      " - 14s - loss: 0.4350 - val_loss: 1.7057\n",
      "\n",
      "Epoch 00028: val_loss improved from 1.71031 to 1.70575, saving model to model.h5\n",
      "Epoch 29/30\n",
      " - 15s - loss: 0.3972 - val_loss: 1.6999\n",
      "\n",
      "Epoch 00029: val_loss improved from 1.70575 to 1.69991, saving model to model.h5\n",
      "Epoch 30/30\n",
      " - 15s - loss: 0.3624 - val_loss: 1.7032\n",
      "\n",
      "Epoch 00030: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9fa478af10>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load a clean dataset\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    \n",
    "    return X\n",
    "\n",
    "\n",
    "# one hot encode target sequence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist = list()\n",
    "    for sequence in sequences:\n",
    "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y = array(ylist)\n",
    "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y\n",
    "\n",
    "\n",
    "# define NMT model\n",
    "def define_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    # compile model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)\n",
    "print('English Max Length: %d' % (eng_length))\n",
    "\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "print('German Vocabulary Size: %d' % ger_vocab_size)\n",
    "print('German Max Length: %d' % (ger_length))\n",
    "\n",
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])\n",
    "trainY = encode_output(trainY, eng_vocab_size)\n",
    "\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])\n",
    "testY = encode_output(testY, eng_vocab_size)\n",
    "\n",
    "# define model\n",
    "model = define_model(ger_vocab_size, eng_vocab_size, ger_length, eng_length, 256)\n",
    "\n",
    "# fit model\n",
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', verbose=1,\n",
    "save_best_only=True, mode='min')\n",
    "\n",
    "model.fit(trainX, trainY, epochs=30, batch_size=64, validation_data=(testX, testY),\n",
    "callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture11.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "src=[das ist in ordnung], target=[thats okay], predicted=[this is]\n",
      "src=[tom a], target=[tom ate], predicted=[tom ate]\n",
      "src=[wir sind alt], target=[were old], predicted=[were old]\n",
      "src=[wie kalt ist es], target=[how cold is it], predicted=[how bad is it]\n",
      "src=[nimm meine], target=[take mine], predicted=[take mine]\n",
      "src=[bring wein], target=[bring wine], predicted=[get wine]\n",
      "src=[ich las lippen], target=[i read lips], predicted=[i read lips]\n",
      "src=[ich kummere mich um tom], target=[ill take tom], predicted=[ill tell tom]\n",
      "src=[ich habe dafur gesorgt], target=[i saw to it], predicted=[i saw to it]\n",
      "src=[ich rannte], target=[i ran], predicted=[i ran]\n",
      "BLEU-1: 0.882148\n",
      "BLEU-2: 0.835760\n",
      "BLEU-3: 0.729296\n",
      "BLEU-4: 0.398758\n",
      "test\n",
      "src=[meine lungen schmerzen], target=[my lungs hurt], predicted=[my very hurts]\n",
      "src=[tom hat gegessen], target=[tom ate], predicted=[tom ate one]\n",
      "src=[gib mal gas], target=[get a move on], predicted=[go to me]\n",
      "src=[ich habe eine arbeit], target=[ive got a job], predicted=[i have a job]\n",
      "src=[schneide ihn in zwei teile], target=[cut it in half], predicted=[cut it in half]\n",
      "src=[nimm meine hand], target=[grab my hand], predicted=[take my rifle]\n",
      "src=[du bist verargert], target=[youre upset], predicted=[youre upset]\n",
      "src=[tom stottert], target=[tom stutters], predicted=[tom bowed]\n",
      "src=[tom ertrank], target=[tom drowned], predicted=[tom bowed]\n",
      "src=[halt das mal], target=[hold this], predicted=[listen that]\n",
      "BLEU-1: 0.558820\n",
      "BLEU-2: 0.436852\n",
      "BLEU-3: 0.354228\n",
      "BLEU-4: 0.162863\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a model\n",
    "\n",
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def load_clean_sentences(filename):\n",
    "    return load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# max sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)\n",
    "# encode and pad sequences\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    X = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    X = pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "\n",
    "# generate target given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction = model.predict(source, verbose=0)[0]\n",
    "    integers = [argmax(vector) for vector in prediction]\n",
    "    target = list()\n",
    "    for i in integers:\n",
    "        word = word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "            \n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    "\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, sources, raw_dataset, eng_tokenizer):\n",
    "    actual, predicted = list(), list()\n",
    "    for i, source in enumerate(sources):\n",
    "        # translate encoded source text\n",
    "        source = source.reshape((1, source.shape[0]))\n",
    "        translation = predict_sequence(model, eng_tokenizer, source)\n",
    "        raw_target, raw_src,_ = raw_dataset[i]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "        \n",
    "    # calculate BLEU score\n",
    "\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "    \n",
    "# load datasets\n",
    "dataset = load_clean_sentences('english-german-both.pkl')\n",
    "train = load_clean_sentences('english-german-train.pkl')\n",
    "test = load_clean_sentences('english-german-test.pkl')\n",
    "# prepare english tokenizer\n",
    "eng_tokenizer = create_tokenizer(dataset[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "eng_length = max_length(dataset[:, 0])\n",
    "# prepare german tokenizer\n",
    "ger_tokenizer = create_tokenizer(dataset[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "ger_length = max_length(dataset[:, 1])\n",
    "# prepare data\n",
    "trainX = encode_sequences(ger_tokenizer, ger_length, train[:, 1])\n",
    "testX = encode_sequences(ger_tokenizer, ger_length, test[:, 1])\n",
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# test on some training sequences\n",
    "print('train')\n",
    "evaluate_model(model, trainX, train, eng_tokenizer)\n",
    "# test on some test sequences\n",
    "print('test')\n",
    "evaluate_model(model, testX, test, eng_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    " Data Cleaning. Different data cleaning operations could be performed on the data, such\n",
    "as not removing punctuation or normalizing case, or perhaps removing duplicate English\n",
    "phrases.\n",
    "\n",
    " Vocabulary. The vocabulary could be refined, perhaps removing words used less than 5\n",
    "or 10 times in the dataset and replaced with unk.\n",
    "\n",
    " More Data. The dataset used to fit the model could be expanded to 50,000, 100,000\n",
    "phrases, or more.\n",
    "\n",
    " Input Order. The order of input phrases could be reversed, which has been reported to\n",
    "lift skill, or a Bidirectional input layer could be used.\n",
    "\n",
    " Layers. The encoder and/or the decoder models could be expanded with additional layers\n",
    "and trained for more epochs, providing more representational capacity for the model.\n",
    "Units. The number of memory units in the encoder and decoder could be increased,\n",
    "providing more representational capacity for the model.\n",
    "\n",
    "Regularization. The model could use regularization, such as weight or activation\n",
    "regularization, or the use of dropout on the LSTM layers.\n",
    "\n",
    " Pre-Trained Word Vectors. Pre-trained word vectors could be used in the model.\n",
    "\n",
    "\n",
    " Alternate Measure. Explore alternate performance measures beside BLEU such as\n",
    "ROGUE. Compare scores for the same translations to develop an intuition for how the\n",
    "measures differ in practice.\n",
    "\n",
    " Recursive Model. A recursive formulation of the model could be used where the next\n",
    "word in the output sequence could be conditional on the input sequence and the output\n",
    "sequence generated so far.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
