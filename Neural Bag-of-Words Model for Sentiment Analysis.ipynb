{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words\n",
    "\n",
    "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in\n",
    "modeling, such as with machine learning algorithms. The approach is very simple and \n",
    "exible,\n",
    "and can be used in a myriad of ways for extracting features from documents. A bag-of-words is\n",
    "a representation of text that describes the occurrence of words within a document. It involves\n",
    "two things:\n",
    " A vocabulary of known words.\n",
    " A measure of the presence of known words.\n",
    "It is called a bag-of-words , because any information about the order or structure of words\n",
    "in the document is discarded. The model is only concerned with whether known words occur in\n",
    "the document, not where in the document.\n",
    "\n",
    "The intuition is that documents are similar if they have similar content. Further, that from\n",
    "the content alone we can learn something about the meaning of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Collect Data\n",
    "### Step 2: Design the Vocabulary\n",
    "### Step 3: Create Document Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Polarity Dataset (review polarity.tar.gz, 3MB).\n",
    "\n",
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.\n",
    "tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Vocabulary\n",
    "\n",
    "As the vocabulary size increases, so does the vector representation of documents. In the previous\n",
    "example, the length of the document vector is equal to the number of known words. You can\n",
    "imagine that for a very large corpus, such as thousands of books, that the length of the vector\n",
    "might be thousands or millions of positions. Further, each document may contain very few of\n",
    "the known words in the vocabulary.\n",
    "\n",
    "This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n",
    "Sparse vectors require more memory and computational resources when modeling and the\n",
    "vast number of positions or dimensions can make the modeling process very challenging for\n",
    "traditional algorithms. As such, there is pressure to decrease the size of the vocabulary when\n",
    "using a bag-of-words model.\n",
    "\n",
    "There are simple text cleaning techniques that can be used as a first step, such as:\n",
    "\n",
    " Ignoring case.\n",
    "\n",
    " Ignoring punctuation.\n",
    "\n",
    " Ignoring frequent words that don't contain much information, called stop words, like a, of,\n",
    "etc.\n",
    "\n",
    " Fixing misspelled words.\n",
    "\n",
    " Reducing words to their stem (e.g. play from playing) using stemming algorithms.\n",
    "\n",
    "A more sophisticated approach is to create a vocabulary of grouped words. This both\n",
    "changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more\n",
    "meaning from the document. In this approach, each word or token is called a gram. Creating a\n",
    "vocabulary of two-word pairs is, in turn, called a bigram model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An n-gram is an n-token sequence of words: a 2-gram (more commonly called a\n",
    "bigram) is a two-word sequence of words like \\please turn\", \\turn your\", or \\your\n",
    "homework\", and a 3-gram (more commonly called a trigram) is a three-word sequence\n",
    "of words like \\please turn your\", or \\turn your homework\".\n",
    "\n",
    "\n",
    "| Page 85, Speech and Language Processing, 2009."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Words\n",
    "Once a vocabulary has been chosen, the occurrence of words in example documents needs to be\n",
    "scored. In the worked example, we have already seen one very simple approach to scoring: a\n",
    "binary scoring of the presence or absence of words. Some additional simple scoring methods\n",
    "include:\n",
    "    \n",
    " Counts. Count the number of times each word appears in a document.\n",
    "\n",
    " Frequencies. Calculate the frequency that each word appears in a document out of all\n",
    "the words in the document.\n",
    "\n",
    "\n",
    "\n",
    "### Word Hashing\n",
    "We can use a hash representation of known words in our vocabulary. This addresses the problem of having a very large vocabulary\n",
    "for a large text corpus because we can choose the size of the hash space, which is in turn the\n",
    "size of the vector representation of the document.\n",
    "\n",
    "Words are hashed deterministically to the same integer index in the target hash space. A\n",
    "binary score or count can then be used to score the word. This is called the hash trick or feature\n",
    "hashing. The challenge is to choose a hash space to accommodate the chosen vocabulary size to\n",
    "minimize the probability of collisions and trade-of sparsity.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Limitations of Bag-of-Words\n",
    "\n",
    "\n",
    "## TF-IDF\n",
    "A problem with scoring word frequency is that highly frequent words start to dominate in the\n",
    "document (e.g. larger score), but may not contain as much informational content to the model\n",
    "as rarer but perhaps domain specific words. One approach is to rescale the frequency of words\n",
    "by how often they appear in all documents, so that the scores for frequent words like the that\n",
    "are also frequent across all documents are penalized. This approach to scoring is called Term\n",
    "Frequency - Inverse Document Frequency, or TF-IDF for short, where:\n",
    "    \n",
    " Term Frequency: is a scoring of the frequency of the word in the current document.\n",
    "    \n",
    " Inverse Document Frequency: is a scoring of how rare the word is across documents.\n",
    "    \n",
    "The scores are a weighting where not all words are equally as important or interesting. The\n",
    "scores have the effect of highlighting words that are distinct (contain useful information) in a\n",
    "given document. Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be\n",
    "low.\n",
    "| Page 118, An Introduction to Information Retrieval, 2008. \n",
    "\n",
    "\n",
    "### Limitations of Bag-of-Words\n",
    "The bag-of-words model is very simple to understand and implement and o\u000b",
    "ers a lot of \n",
    "exibility\n",
    "for customization on your specific text data. It has been used with great success on prediction\n",
    "problems like language modeling and documentation classi\f",
    "cation. Nevertheless, it su\u000b",
    "ers from\n",
    "some shortcomings, such as:\n",
    "\n",
    " Vocabulary: The vocabulary requires careful design, most specifically in order to manage\n",
    "the size, which impacts the sparsity of the document representations.\n",
    "\n",
    " Sparsity: Sparse representations are harder to model both for computational reasons\n",
    "(space and time complexity) and also for information reasons, where the challenge is for\n",
    "the models to harness so little information in such a large representational space.\n",
    "\n",
    " Meaning: Discarding word order ignores the context, and in turn meaning of words in\n",
    "the document (semantics). Context and meaning can offer a lot to the model, that if\n",
    "modeled could tell the di\u000b",
    "erence between the same words differently arranged (this is\n",
    "interesting vs is this interesting), synonyms (old bike vs used bike), and much more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a Neural Bag-of-Words Model for Sentiment Analysis\n",
    "\n",
    "\n",
    "1. Movie Review Dataset\n",
    "2. Data Preparation\n",
    "3. Bag-of-Words Representation\n",
    "4. Sentiment Analysis Models\n",
    "5. Comparing Word Scoring Methods\n",
    "6. Predicting Sentiment for New Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Vocabulary\n",
    "\n",
    "It is important to define a vocabulary of known words when using a bag-of-words model. The\n",
    "more words, the larger the representation of documents, therefore it is important to constrain\n",
    "the words to only those believed to be predictive. This is difficult to know beforehand and often\n",
    "it is important to test different hypotheses about how to construct a useful vocabulary. We\n",
    "have already seen how we can remove punctuation and numbers from the vocabulary in the\n",
    "previous section. \n",
    "\n",
    "We can repeat this for all documents and build a set of all known words.\n",
    "\n",
    "\n",
    "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and\n",
    "their count that allows us to easily update and query. \n",
    "\n",
    "\n",
    "Each document can be added to the counter (a new function called add doc to vocab()) and we can step over all of the reviews in the negative directory and then the positive directory (a new function called process docs())."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['films', 'adapted', 'comic', 'books', 'plenty', 'success', 'whether', 'theyre', 'superheroes', 'batman', 'superman', 'spawn', 'geared', 'toward', 'kids', 'casper', 'arthouse', 'crowd', 'ghost', 'world', 'theres', 'never', 'really', 'comic', 'book', 'like', 'hell', 'starters', 'created', 'alan', 'moore', 'eddie', 'campbell', 'brought', 'medium', 'whole', 'new', 'level', 'mid', 'series', 'called', 'watchmen', 'say', 'moore', 'campbell', 'thoroughly', 'researched', 'subject', 'jack', 'ripper', 'would', 'like', 'saying', 'michael', 'jackson', 'starting', 'look', 'little', 'odd', 'book', 'graphic', 'novel', 'pages', 'long', 'includes', 'nearly', 'consist', 'nothing', 'footnotes', 'words', 'dont', 'dismiss', 'film', 'source', 'get', 'past', 'whole', 'comic', 'book', 'thing', 'might', 'find', 'another', 'stumbling', 'block', 'hells', 'directors', 'albert', 'allen', 'hughes', 'getting', 'hughes', 'brothers', 'direct', 'seems', 'almost', 'ludicrous', 'casting', 'carrot', 'top', 'well', 'anything', 'riddle', 'better', 'direct', 'film', 'thats', 'set', 'ghetto', 'features', 'really', 'violent', 'street', 'crime', 'mad', 'geniuses', 'behind', 'menace', 'ii', 'society', 'ghetto', 'question', 'course', 'whitechapel', 'londons', 'east', 'end', 'filthy', 'sooty', 'place', 'whores', 'called', 'unfortunates', 'starting', 'get', 'little', 'nervous', 'mysterious', 'psychopath', 'carving', 'profession', 'surgical', 'precision', 'first', 'stiff', 'turns', 'copper', 'peter', 'godley', 'robbie', 'coltrane', 'world', 'enough', 'calls', 'inspector', 'frederick', 'abberline', 'johnny', 'depp', 'blow', 'crack', 'case', 'abberline', 'widower', 'prophetic', 'dreams', 'unsuccessfully', 'tries', 'quell', 'copious', 'amounts', 'absinthe', 'opium', 'upon', 'arriving', 'whitechapel', 'befriends', 'unfortunate', 'named', 'mary', 'kelly', 'heather', 'graham', 'say', 'isnt', 'proceeds', 'investigate', 'horribly', 'gruesome', 'crimes', 'even', 'police', 'surgeon', 'cant', 'stomach', 'dont', 'think', 'anyone', 'needs', 'briefed', 'jack', 'ripper', 'wont', 'go', 'particulars', 'say', 'moore', 'campbell', 'unique', 'interesting', 'theory', 'identity', 'killer', 'reasons', 'chooses', 'slay', 'comic', 'dont', 'bother', 'cloaking', 'identity', 'ripper', 'screenwriters', 'terry', 'hayes', 'vertical', 'limit', 'rafael', 'yglesias', 'les', 'mis', 'rables', 'good', 'job', 'keeping', 'hidden', 'viewers', 'end', 'funny', 'watch', 'locals', 'blindly', 'point', 'finger', 'blame', 'jews', 'indians', 'englishman', 'could', 'never', 'capable', 'committing', 'ghastly', 'acts', 'hells', 'ending', 'whistling', 'stonecutters', 'song', 'simpsons', 'days', 'holds', 'back', 'electric', 'carwho', 'made', 'steve', 'guttenberg', 'star', 'dont', 'worry', 'itll', 'make', 'sense', 'see', 'onto', 'hells', 'appearance', 'certainly', 'dark', 'bleak', 'enough', 'surprising', 'see', 'much', 'looks', 'like', 'tim', 'burton', 'film', 'planet', 'apes', 'times', 'seems', 'like', 'sleepy', 'hollow', 'print', 'saw', 'wasnt', 'completely', 'finished', 'color', 'music', 'finalized', 'comments', 'marilyn', 'manson', 'cinematographer', 'peter', 'deming', 'dont', 'say', 'word', 'ably', 'captures', 'dreariness', 'victorianera', 'london', 'helped', 'make', 'flashy', 'killing', 'scenes', 'remind', 'crazy', 'flashbacks', 'twin', 'peaks', 'even', 'though', 'violence', 'film', 'pales', 'comparison', 'blackandwhite', 'comic', 'oscar', 'winner', 'martin', 'childs', 'shakespeare', 'love', 'production', 'design', 'turns', 'original', 'prague', 'surroundings', 'one', 'creepy', 'place', 'even', 'acting', 'hell', 'solid', 'dreamy', 'depp', 'turning', 'typically', 'strong', 'performance', 'deftly', 'handling', 'british', 'accent', 'ians', 'holm', 'joe', 'goulds', 'secret', 'richardson', 'dalmatians', 'log', 'great', 'supporting', 'roles', 'big', 'surprise', 'graham', 'cringed', 'first', 'time', 'opened', 'mouth', 'imagining', 'attempt', 'irish', 'accent', 'actually', 'wasnt', 'half', 'bad', 'film', 'however', 'good', 'strong', 'violencegore', 'sexuality', 'language', 'drug', 'content']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import re\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "# load the document\n",
    "filename = 'txt_sentoken/pos/cv000_29590.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "    # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "    \n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# keep tokens with a min occurrence\n",
    "min_occurrence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Word Scoring Methods\n",
    "The texts to matrix() function for the Tokenizer in the Keras API provides 4 di\u000b",
    "erent\n",
    "methods for scoring words; they are:\n",
    "\n",
    " Binary Where words are marked as present (1) or absent (0)\n",
    "\n",
    " Count Where the occurrence count for each word is marked as an integer\n",
    "\n",
    " TFIDF Where each word is scored based on their frequency, where words that are common\n",
    "across all documents are penalized\n",
    "\n",
    " FREQ Where words are scored based on their frequency of occurrence within the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.python.keras import backend as k\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "#     plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    tf.keras.utils.plot_model(\n",
    "        model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# evaluate a neural network model\n",
    "def evaluate_mode(Xtrain, ytrain, Xtest, ytest):\n",
    "    scores = list()\n",
    "    n_repeats = 10\n",
    "    n_words = Xtest.shape[1]\n",
    "    for i in range(n_repeats):\n",
    "        # define network\n",
    "        model = define_model(n_words)\n",
    "        # fit network\n",
    "        model.fit(Xtrain, ytrain, epochs=10, verbose=0)\n",
    "        # evaluate\n",
    "        _, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "        scores.append(acc)\n",
    "        print('%d accuracy: %s' % ((i+1), acc))\n",
    "    return scores\n",
    "# prepare bag of words encoding of docs\n",
    "def prepare_data(train_docs, test_docs, mode):\n",
    "    # create the tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    # fit the tokenizer on the documents\n",
    "    tokenizer.fit_on_texts(train_docs)\n",
    "    # encode training data set\n",
    "    Xtrain = tokenizer.texts_to_matrix(train_docs, mode=mode)\n",
    "    # encode training data set\n",
    "    Xtest = tokenizer.texts_to_matrix(test_docs, mode=mode)\n",
    "    return Xtrain, Xtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_82 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.925\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.93\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_86 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.94\n",
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_88 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.93\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.93\n",
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_92 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.935\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_94 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.935\n",
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.93\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_98 (Dense)             (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.925\n",
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_100 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.925\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.9\n",
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_104 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.89\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_106 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 accuracy: 0.905\n",
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.905\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_110 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.905\n",
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_112 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.89\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.9\n",
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_116 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.9\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_118 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.895\n",
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.895\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_122 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.87\n",
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_124 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.86\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_126 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.88\n",
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_128 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.85\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_130 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.88\n",
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_132 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_133 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 accuracy: 0.875\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_134 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.895\n",
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_136 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.88\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_138 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_139 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "9 accuracy: 0.89\n",
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_140 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.88\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_142 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1 accuracy: 0.875\n",
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_144 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_145 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "2 accuracy: 0.855\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_146 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_147 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "3 accuracy: 0.865\n",
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_148 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_149 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "4 accuracy: 0.87\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_150 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_151 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "5 accuracy: 0.875\n",
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_152 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "6 accuracy: 0.87\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_154 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "7 accuracy: 0.86\n",
      "Model: \"sequential_78\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_156 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_157 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "8 accuracy: 0.87\n",
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_158 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_159 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 accuracy: 0.87\n",
      "Model: \"sequential_80\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_160 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_161 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10 accuracy: 0.85\n",
      "          binary      count      tfidf       freq\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.930500   0.898500   0.876000   0.866000\n",
      "std     0.004972   0.005798   0.013292   0.008433\n",
      "min     0.925000   0.890000   0.850000   0.850000\n",
      "25%     0.926250   0.895000   0.871250   0.861250\n",
      "50%     0.930000   0.900000   0.880000   0.870000\n",
      "75%     0.933750   0.903750   0.880000   0.870000\n",
      "max     0.940000   0.905000   0.895000   0.875000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pyplot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-18fcde0002c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pyplot' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.get_default_graph()\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# run experiment\n",
    "modes = ['binary', 'count', 'tfidf', 'freq']\n",
    "results = pd.DataFrame()\n",
    "for mode in modes:\n",
    "    # prepare data for mode\n",
    "    Xtrain, Xtest = prepare_data(train_docs, test_docs, mode)\n",
    "    # evaluate model on data for mode\n",
    "    results[mode] = evaluate_mode(Xtrain, ytrain, Xtest, ytest)\n",
    "# summarize results\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "import matplotlib.pyplot as plt\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          binary      count      tfidf       freq\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    0.930500   0.898500   0.876000   0.866000\n",
      "std     0.004972   0.005798   0.013292   0.008433\n",
      "min     0.925000   0.890000   0.850000   0.850000\n",
      "25%     0.926250   0.895000   0.871250   0.861250\n",
      "50%     0.930000   0.900000   0.880000   0.870000\n",
      "75%     0.933750   0.903750   0.880000   0.870000\n",
      "max     0.940000   0.905000   0.895000   0.875000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot\n",
    "print(results.describe())\n",
    "# plot results\n",
    "results.boxplot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on new movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_166 (Dense)            (None, 50)                1288450   \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 1,288,501\n",
      "Trainable params: 1,288,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 2000 samples\n",
      "Epoch 1/10\n",
      "2000/2000 - 1s - loss: 0.4558 - accuracy: 0.7800\n",
      "Epoch 2/10\n",
      "2000/2000 - 1s - loss: 0.0507 - accuracy: 0.9955\n",
      "Epoch 3/10\n",
      "2000/2000 - 1s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "2000/2000 - 1s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "2000/2000 - 1s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "2000/2000 - 1s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "2000/2000 - 1s - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "2000/2000 - 1s - loss: 9.3082e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "2000/2000 - 1s - loss: 7.1239e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "2000/2000 - 1s - loss: 5.6146e-04 - accuracy: 1.0000\n",
      "Review: [Best movie ever! It was great, I recommend it.]\n",
      "Sentiment: POSITIVE (54.781%)\n",
      "Review: [This is a bad movie.]\n",
      "Sentiment: NEGATIVE (64.227%)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "    \n",
    "# load doc, clean and return line of tokens\n",
    "def doc_to_line(filename, vocab):\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        # add to list\n",
    "        lines.append(line)\n",
    "    return lines\n",
    "    \n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "    \n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# define the model\n",
    "def define_model(n_words):\n",
    "    # define network\n",
    "    model = Sequential()\n",
    "    model.add(Dense(50, input_shape=(n_words,), activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "#     tf.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # encode\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='binary')\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab)\n",
    "test_docs, ytest = load_clean_dataset(vocab)\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "\n",
    "# encode data\n",
    "Xtrain = tokenizer.texts_to_matrix(train_docs, mode='binary')\n",
    "Xtest = tokenizer.texts_to_matrix(test_docs, mode='binary')\n",
    "\n",
    "# define network\n",
    "n_words = Xtrain.shape[1]\n",
    "model = define_model(n_words)\n",
    "\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "\n",
    "# test positive text\n",
    "text = 'Best movie ever! It was great, I recommend it.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "# test negative text\n",
    "text = 'This is a bad movie.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps...\n",
    "\n",
    "Manage Vocabulary. Explore using a larger or smaller vocabulary. Perhaps you can\n",
    "get better performance with a smaller set of words.\n",
    "\n",
    " **Tune the Network Topology**. Explore alternate network topologies such as deeper or\n",
    "wider networks. Perhaps you can get better performance with a more suited network.\n",
    "\n",
    " **Use Regularization**. Explore the use of regularization techniques, such as dropout.\n",
    "Perhaps you can delay the convergence of the model and achieve better test set performance.\n",
    "\n",
    "\n",
    "**More Data Cleaning**. Explore more or less cleaning of the review text and see how it\n",
    "impacts the model skill.\n",
    "\n",
    " **Training Diagnostics**. Use the test dataset as a validation dataset during training and\n",
    "create plots of train and test loss. Use these diagnostics to tune the batch size and number\n",
    "of training epochs.\n",
    "\n",
    " **Trigger Words**. Explore whether there are speci\f",
    "c words in reviews that are highly\n",
    "predictive of the sentiment.\n",
    "\n",
    " **Use Bigrams**. Prepare the model to score bigrams of words and evaluate the performance\n",
    "under different scoring schemes.\n",
    "\n",
    " **Truncated Reviews**. Explore how using a truncated version of the movie reviews results\n",
    "impacts model skill, try truncating the start, end and middle of reviews.\n",
    "\n",
    " **Ensemble Models**. Create models with di\u000b",
    "erent word scoring schemes and see if using\n",
    "ensembles of the models results in improves to model skill.\n",
    "\n",
    " **Real Reviews**. Train a final model on all data and evaluate the model on real movie\n",
    "reviews taken from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/derrelldsouza/imdb-sentiment-analysis-eda-ml-lstm-bert"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
