{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Models for Document Classification\n",
    "\n",
    "\n",
    "# Word Embeddings + CNN = Text Classification\n",
    "\n",
    "\n",
    "The modus operandi for text classification involves the use of a word embedding for representing\n",
    "words and a Convolutional Neural Network (CNN) for learning how to discriminate documents\n",
    "on classification problems. Yoav Goldberg, in his primer on deep learning for natural language\n",
    "processing, comments that neural networks in general o\u000b",
    "er better performance than classical\n",
    "linear classifiers, especially when used with pre-trained word embeddings.\n",
    "\n",
    "\n",
    "The architecture is therefore comprised of three key pieces:\n",
    "    \n",
    " Word Embedding: A distributed representation of words where different words that\n",
    "have a similar meaning (based on their usage) also have a similar representation.\n",
    "\n",
    " Convolutional Model: A feature extraction model that learns to extract salient features\n",
    "from documents represented using a word embedding.\n",
    "\n",
    "\n",
    " Fully Connected Model: The interpretation of extracted features in terms of a predictive\n",
    "output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use a Single Layer CNN Architecture\n",
    "\n",
    "\n",
    "You can get good results for document classification with a single layer CNN, perhaps with\n",
    "differently sized kernels across the filters to allow grouping of word representations at different\n",
    "scales. \n",
    "\n",
    "Yoon Kim in his study of the use of pre-trained word vectors for classification tasks with\n",
    "Convolutional Neural Networks found that using pre-trained static word vectors does very well.\n",
    "\n",
    "\n",
    "He suggests that pre-trained word embeddings that were trained on very large text corpora,\n",
    "such as the freely available Word2Vec vectors trained on 100 billion tokens from Google news\n",
    "may offer good universal features for use in natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "He also discovered that further task-specific tuning of the word vectors offer a small additional\n",
    "improvement in performance. Kim describes the general approach of using CNN for natural\n",
    "language processing. Sentences are mapped to embedding vectors and are available as a matrix\n",
    "input to the model. Convolutions are performed across the input word-wise using differently\n",
    "sized kernels, such as 2 or 3 words at a time. The resulting feature maps are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IPython.core.display.Image"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.display.Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture is based on the approach used by Ronan Collobert, et al. in their paper\n",
    "Natural Language Processing (almost) from Scratch, 2011. In it, they develop a single end-to-end\n",
    "neural network model with convolutional and pooling layers for use across a range of fundamental\n",
    "natural language processing problems. Kim provides a diagram that helps to see the sampling\n",
    "of the filters using differently sized kernels as different colors (red and yellow)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Usefully, he reports his chosen model configuration, discovered via grid search and used\n",
    "across a suite of 7 text classification tasks, summarized as follows***:\n",
    "    \n",
    "Transfer function: rectified linear.\n",
    " Kernel sizes: 2, 4, 5.\n",
    "\n",
    " Number of filters: 100. \n",
    "\n",
    " Dropout rate: 0.5.\n",
    "\n",
    " Weight regularization (L2): 3.\n",
    "\n",
    " Batch Size: 50.\n",
    "\n",
    " Update Rule: Adadelta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dial in CNN Hyperparameters\n",
    "\n",
    "\n",
    "Some hyperparameters matter more than others when tuning a convolutional neural network on your document classification problem. \n",
    "\n",
    "Ye Zhang and Byron Wallace performed a sensitivity analysis into the hyperparameters needed to configure a single layer convolutional neural network for document classification. \n",
    "\n",
    "\n",
    "The study is motivated by their claim that the models are sensitive to their configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture2.png)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The study makes a number of useful findings that could be used as a starting point for\n",
    "configuring shallow CNN models for text classification. The general findings were as follows:\n",
    "    \n",
    " The choice of pre-trained Word2Vec and GloVe embeddings differ from problem to problem,\n",
    "and both performed better than using one hot encoded word vectors.\n",
    " The size of the kernel is important and should be tuned for each problem.\n",
    " The number of feature maps is also important and should be tuned.\n",
    " The 1-max pooling generally outperformed other types of pooling.\n",
    " Dropout has little effect on the model performance.\n",
    "They go on to provide more specific heuristics, as follows:\n",
    " Use Word2Vec or GloVe word embeddings as a starting point and tune them while fitting\n",
    "the model.\n",
    " Grid search across different kernel sizes to find the optimal configuration for your problem,\n",
    "in the range 1-10.\n",
    "Search the number of filters from 100-600 and explore a dropout of 0.0-0.5 as part of the\n",
    "same search.\n",
    " Explore using Tanh, ReLU, and linear activation functions.\n",
    "The key caveat is that the findings are based on empirical results on binary text classification\n",
    "problems using single sentences as input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Character-Level CNNs\n",
    "\n",
    "\n",
    "Text documents can be modeled at the character level using convolutional neural networks\n",
    "that are capable of learning the relevant hierarchical structure of words, sentences, paragraphs,\n",
    "and more. Xiang Zhang, et al. use a character-based representation of text as input for a\n",
    "convolutional neural network. The promise of the approach is that all of the labor-intensive\n",
    "effort required to clean and prepare text could be overcome if a CNN can learn to abstract the\n",
    "salient details.\n",
    "\n",
    "The model reads in one hot encoded characters in a \f",
    "xed-sized alphabet. Encoded characters\n",
    "are read in blocks or sequences of 1,024 characters. A stack of 6 convolutional layers with\n",
    "pooling follows, with 3 fully connected layers at the output end of the network in order to make\n",
    "a prediction.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consider Deeper CNNs for Classification\n",
    "\n",
    "\n",
    "Better performance can be achieved with very deep convolutional neural networks, although\n",
    "standard and reusable architectures have not been adopted for classification tasks, yet. Alexis\n",
    "Conneau, et al. comment on the relatively shallow networks used for natural language processing\n",
    "and the success of much deeper networks used for computer vision applications. For example,\n",
    "Kim (above) restricted the model to a single convolutional layer.\n",
    "Other architectures used for natural language reviewed in the paper are limited to 5 and 6\n",
    "layers. These are contrasted with successful architectures used in computer vision with 19 or\n",
    "even up to 152 layers. They suggest and demonstrate that there are benefits for hierarchical\n",
    "feature learning with very deep convolutional neural network model, called VDCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results on a suite of 8 large text classification tasks show better performance than more\n",
    "shallow networks. Specifically, state-of-the-art results on all but two of the datasets tested,\n",
    "at the time of writing. Generally, they make some key findings from exploring the deeper\n",
    "architectural approach:\n",
    "\n",
    " The very deep architecture worked well on small and large datasets.\n",
    "\n",
    " Deeper networks decrease classification error.\n",
    "\n",
    " Max-pooling achieves better results than other, more sophisticated types of pooling.\n",
    "\n",
    " Generally going deeper degrades accuracy; the shortcut connections used in the architecture\n",
    "are important. \n",
    "\n",
    "This is the first time that the \\benefit of depths\" was shown for convolutional\n",
    "neural networks in NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Vocabulary\n",
    "\n",
    "It is important to define a vocabulary of known words when using a text model. The more\n",
    "words, the larger the representation of documents, therefore it is important to constrain the\n",
    "words to only those believed to be predictive. This is di\u000ecult to know beforehand and often it\n",
    "is important to test different hypotheses about how to construct a useful vocabulary. We have\n",
    "already seen how we can remove punctuation and numbers from the vocabulary in the previous\n",
    "section. We can repeat this for all documents and build a set of all known words.\n",
    "We can develop a vocabulary as a Counter, which is a dictionary mapping of words and\n",
    "their count that allows us to easily update and query. Each document can be added to the\n",
    "counter (a new function called add doc to vocab()) and we can step over all of the reviews in\n",
    "the negative directory and then the positive directory (a new function called process docs()).\n",
    "The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44276\n",
      "[('film', 7983), ('one', 4946), ('movie', 4826), ('like', 3201), ('even', 2262), ('good', 2080), ('time', 2041), ('story', 1907), ('films', 1873), ('would', 1844), ('much', 1824), ('also', 1757), ('characters', 1735), ('get', 1724), ('character', 1703), ('two', 1643), ('first', 1588), ('see', 1557), ('way', 1515), ('well', 1511), ('make', 1418), ('really', 1407), ('little', 1351), ('life', 1334), ('plot', 1288), ('people', 1269), ('could', 1248), ('bad', 1248), ('scene', 1241), ('movies', 1238), ('never', 1201), ('best', 1179), ('new', 1140), ('scenes', 1135), ('man', 1131), ('many', 1130), ('doesnt', 1118), ('know', 1092), ('dont', 1086), ('hes', 1024), ('great', 1014), ('another', 992), ('action', 985), ('love', 977), ('us', 967), ('go', 952), ('director', 948), ('end', 946), ('something', 945), ('still', 936)]\n",
      "25767\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "    \n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab):\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "\n",
    "    \n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('txt_sentoken/pos', vocab)\n",
    "process_docs('txt_sentoken/neg', vocab)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "min_occurrence = 2\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurrence]\n",
    "print(len(tokens))\n",
    "\n",
    "def save_list(lines, filename):\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "    # save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Review Polarity Dataset (review polarity.tar.gz, 3MB).\n",
    "\n",
    "\n",
    "https://raw.githubusercontent.com/jbrownlee/Datasets/master/review_polarity.\n",
    "tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop an Embedding + CNN Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Note: The preparation of the movie review dataset was first described in Chapter 9. In this\n",
    "section, we will look at 3 things:\n",
    "    \n",
    "1. Separation of data into training and test sets.\n",
    "\n",
    "2. Loading and cleaning the data to remove punctuation and numbers.\n",
    "\n",
    "3. Defining a vocabulary of preferred words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad bad bad one word seems pretty much sums beyond valley dolls summary isnt enough ta ta ta still havent got point director russ meyers predilection casting attractive large breasted women ultimately expose aforementioned areas really one reason recommend even taking look movie fact cowritten famed film critic roger ebert also responsible screenplay watching movie never able sit another one reviews gives movie thumbs bad writing straight face movie stinks loud quite frankly movie deserves parts bad almost funny im giving maybe generous right opening credits knew bomb hands way credits actually shot distracting first scene see includes big breasted young woman chased guy nazi uniform absolutely idea hell happening get explained later soon first scene cut completely unrelated scene honest sat movie mesmerized incredibly awful actually forgot seemingly place opening popped later film quality writing rest film wouldnt surprised opening never explained movie ask like really matters ok goes band headed kelly macnamara dolly reed friends go hollywood try gain foothold music industry manage find success due much hooters anything else sure wasnt brutally bad singing voices movie chronicles lives change worse pressures fame get everything big egos booze drugs free flowing sex sends downward spiral couple idiotic subplots thrown good measure fame one pretty much sums thing creative standpoint nothing redeeming obsession big russ meyer seemed dialogue incredibly bad literally funny parts mr ebert generously thrown hey man dig time favorite happening freaks ask lines like go wrong ebert tried inject many big words possible dialogue maybe thought would make movie seem smarter dont know big words world wouldnt able disguise bad writing even worse acting wretched dialogue goes along well wretched quality everything else movie ive seen home movies directed better meyer managed turkey fact one scene one van driving hollywood make fortunes really question meyer editors suffered serious head injuries add directing writing music movie almost got check sound system see broken pile crap speakers cast first lets start david played harris manager band got looking guy ever set foot front motion picture camera sadly acting doesnt come close making looks following along point shouldnt surprise meyers stable well endowed girls also benefit fairly attractive go along assets dolly reed plays kelly leader band surprise cast cup size talents yes loose shirt times display impressive talents sadly ass almost large chest hey sexist movie im writing sexist review former playboy playmate cynthia myers fairly small role casey one band members goes along rest idiotic thinking movie meyer casts gorgeous playmate rack kill obviously acting talent nude scenes biggest disappointment sure russ time get artsy throw well placed shadows side fun lesbo scene sound like im writing review porn magazine hey ill admit reason actually managed sit damn movie catch look cynthia myers naked since huge disappointment pretty much wasted two hours life turkey thing say movie stay away unless course want feel good knowing even pulitzer prize winning film critic like roger ebert screwed least life thinking checking double ds better nude cynthia myers pictures internet movie avoided costs even better idea might require video stores place warning box beyond valley dolls beware movie extremely common sense proceed extreme caution', 'isnt ultimate sign movies cinematic ineptitude cant think much say sucks one first official year releases supernova movie cant seem get past adjectives one although boring stupid absurd doesnt amount much review shame would able save chore desperately trying elaborate la vie goes nothing ill keep short suppose first bad omen supernova came director walter hill hours removed name movie requesting replaced pseudonym thomas lee films fate sealed many minds struggling studio mgm declined screen press event usually signifying studios lack confidence particular movie hills actions prudent captain medical space vessel nightingale dies tragic accident reformed drug addict also first officer reason james spader forced take command ship picks distress call nearby planet arrival picks one survivor apparent accident abandoned mining colony one crew members angela bassett knows passenger played peter facinelli bad feelings know means intergalactic hitchhiker carrying mysterious cargo substance purpose unknown though seems bring form pleasure whoever touches another one crew members experiences first hand spending minutes partially inside glob goo impressive evidently touching enigmatic thingie makes younger stronger movie never bothers explain soon enough though spader bassett running around ship like mad chased facinelli honest dont even remember exactly remember didnt care plot suggested less interesting ideas ball goo intergalactic time bomb dropped chance develop anything truly intriguing fact everything dropped actors fun running around looks like elaborate set well effects good though theres hardly studio movie bad special effects days im sure whether thats remarkable accomplishment performances hardly worth talking im even sure call whats performances though angela bassett sure good giving people finger james spader bad actor proves one blandest action stars ive seen mostly isnt given character personality action scenes bland since theyre pretty much rehashes action elements werent particularly entertaining first time around since action scenes everything movie pretty much dead water life cant figure called supernova', 'gordy movie sesame street skit bad one movie stupid dumb depressing think hollywood executives actually gave green light even surprising fact disney movie im sure children target audience movie kids age five may able tolerate story farm named gordy voiced whose family taken away north know means death course hear animals talk actually went trouble attempting sync voices mouths comes terrible actually almost funny way remotely interesting likable character soon appears little girl named jinnie sue young sees gordy back truck essentially steals jinnie country singer film goes huge tangent show little concert people dancing point maybe one producers relatives wanted show camera promote something cut huge social gathering drop another young kid named hank royce sad divorced mother dating leaves party meets jinnie sue accidentally falls pool probably sitting diving board suit nah didnt see one coming starts drown miraculously saved gordy pushes float saves insanely stupid already story quickly changes jinnie gives gordy hank ends becoming ceo food processing corporation hanks grandfather original ceo dies leaves fortune hank gordy course must villain even villain isnt evil never raises voice becomes angry course typical idiot goons kidnap gordy beyond stupid cartoony constantly two steps ahead story hard tell whether overall corniness cheesiness movie intentional family film filmmakers untalented stupid times gordy tolerable watch thus earning one star dreaded unbelievably boring cliche dumb unfunny corny plain bad may scare children certainly disturbed see also babe', 'disconnect phone line dont accept charges anything avoid wretched melodramatic sisterhood dramedy hanging figured needed get touch feminine side hanging seemed like ideal opportunity film features incredible palate female talent capability behind camera brought mind sparkling gems sleepless seattle unsung heroes meg ryan diane keaton lisa kudrow play trio sisters separated career judgments family ties must reunite father walter matthau admitted hospital alzheimers disease may read like optimum opportunity rekindle relationship reflect poignancy past script sisters delia nora ephron exasperating shapeless dreck emotional hanging overall effect telemarketer pestering two hours dont option title suggests first halfhour ephron sisters use telephone conversations basis character development annoying ineffective device ring every five minutes everyone hurriedly rushes along leaving marginal time frustrated viewer relate sisters issues problems hanging apple pie felt getting mere crust story granted genuine sincere moments film help establish remainder strained emotions nothing inferior dramatic muck outrageous strategy hanging series largely attempts character development expected exhibit compassion courtesy toward sisters join melodramatic finale able identify eve ryan open caring daughter one stayed fathers side everyone else moved forward pursue impending career georgia keaton eldest daughter celebrating fifth year anniversary magazine called georgia maddy kudrow soap opera actress spends time either contemplating possible path stardom nursing dog ryans convincing performance diverting cuteness two agreeable aspects hanging kudrow delightfully eccentric offkilter phoebe friends totally wasted ditto keaton serving double shift costar director time slot difficult priority juggle frenzy apparent chick flick lack chuckles reliable matthau reduced chaotic shtick given characters situation seems depressing amusing even peak form humor hanging represented matthaus nasty quips ryans eternal battle aforementioned pooch swallow pill accounts three four chuckles expel film curiosity suddenly tweaked discover promising starstudded approach could turn viciously sour really mystery predictable melodramatic filth hanging certainly fault actresses pin screenplay attempts clear vital issues three four minutes spending rest running time annoying flurry phone conversations certainly far cry one would label rewarding experience hanging least enjoyable wrong number beginning', 'robert forster found famous appearing jackie brown immediately signed little film called american perfekt almost two years ago waited patiently film released never finally forgot day though selection local video store stumbled upon guessed american perfekt immediately rented certain amount glee rushed home view seen film understand never saw theatrical release american perfekt jumbled mess storyline nonexistent took half movie figure going point thing really knew sure movie never going introduce plot sort wants get socalled charm alone doesnt work robert forster plays psychiatrist picks amanda plummer head road together along way discernable destination run sorts kooky wacky characters suppose freewheeling plot style supposed give film element danger make sleepy im trying new things within realm film expense coherence nothing movie makes sense actions characters go unexplained even theyre truly bizarre ill give example without giving much away david thewlis part drifting conman one point midway movie runs forster plummer road car passes see face covered blood think would get explanation happened would think wouldnt american perfekt waste time nothing gained watching movie except maybe rent dont bother trying figure whats going hopes everything resolved end doesnt happen explanation given misspelling perfect either', 'first review post newsgroup kind feel like say something negative film one else seems care takes certain liberties taken historical story however even one thinks fiction prince egypt remains shallow ill begin beginning biggest difference original biblical story version moses semblance divinity bible whereas animated version gives impression reluctant hero maybe knew god side id little bit confidence differences well lack important female characters passing original death like nothing thing though storys focus shifted rather story fatherson prince egypt almost solely relationship rameses moses originally excited story element saw movie came maudlin rameses moses squabble like children interests peace moses saves rameses butt even though moses one started yawn didnt care anymore time moses ran murdering course never happened original story moses exiled enough differences lets talk movie features moses interests making character human lacks divinity whatsoever isnt convincing anyone knows god features cookiecutter rameses least given dignity voicework ralph fiennes schindlers list among things film also features extremely annoying character way miriam voiced sandra bullock even though irritating thing tendency burst song apparent reason bullock replaced singing voice speaking contains annoying music contains shallow writing features extraordinary animation one good thing say film contains attempt massmarket story manifestation god christian got impression blasphemy writers producers took butcher knife story', 'lake placid marks yet another entry series predator pics screen staple late post jaws revived recently godawful anaconda placid claims horrorcomedy directed guy house attempts humor actually less funny deadpan seriousness anaconda paleontologist kelly scott bridget fonda sent maine examine tooth removed body bitten half lake discovering tooth belongs crocodile shouldnt even hemisphere kelly goes game warden jack wells bill pullman sheriff hank brendan gleeson theyre joined unwelcome guest hector oliver platt scholar worships searches world along way merry band meets mrs betty white weird old lady lives lake know expect movie lots shots camera eyes predator croc cam swimming toward someones dangling legs jaws like music plays one character hector whos obsessed croc stupidly endangers rest another character insists predator cant possibly exist unlike slippery cousin anaconda lake placid wants present formulaic plot tongueincheek result neither scary funny tedious director steve miner several horror films resume including two installments friday halloween screenwriter david kelley best known creator tv series like ally mcbeal chicago hope doesnt seem stomach em flick body count surprisingly low doesnt include major characters annoyingly kelly hector insist capture crocodile alive rather killing lake placid manages let discover croc mostly computergenerated course like snake anaconda monster crocodile doesnt seem real moves quickly ways seem unnatural lake placid also offers little explanation giant crocodile maine lot mumbo jumbo really dont know much crocodiles would better going something like urban myth alligators sewers movie like doesnt offer much explanation radioactive mutant creature outer space give audience something hang disbelief pullman fonda seem plodding movie autopilot time theyre probably thinking killing agents wondering making movie marks end careers platt white hand seem giving best efforts manage squeeze chuckles sorry script', 'main problem martin lawrences pet project thin line love hate like fatal attraction variation protagonist man character irresponsible jerk case doesnt seem anything except justify womans actions especially case lawrences darnell wright one macho guys women lined mile long dont think condone im male philosophy one heterosexual males lucky enough get hands beautiful kind girl treat like princess respect darnell doesnt think like sleeps girls dumps period film discovered newest target beautiful wealthy brandi web played nicely whitfield runs successful real estate business kill dumping friend mia king found thinking im supposed feel sorry guy largely darnells fault lining women youd think hed enough common sense think would backfire one day mother says doesnt get sense hes irresponsible also pretty dumb especially get sense hes dumb brandi tells killed husband allegedly abusing id put pants two seconds brandi psycho bitch hard agree says darnell finale guys like treating like garbage one final flaw though letting brandi fall darnell begin brandi classy intelligent woman harvard resists darnells immature play calls woman like would real life sudden says yes listen darnells four letter word vocabulary watch actions wonder intelligent woman would fall guy like darnell period much less unhealthy obsession lawrence good wants order prove needs let people write direct movies look movie four writers result long two hours couldve easily worked ninety minutes lot subplots characters appear disappear quickly came makes think writers didnt get along isnt necessary bright spots though whitfield regina king della reese bobby brown written parts help lot funny parts like example scene reese attempts fight harrassing son damaging property theyre enough sustain film', 'education know happiness starring sylvia chang teresa hu hsu ming li mao directed edward yang written yang cinematography christopher doyle chang every country eventually new wave france vague brazil cinema china fifth generation waves take longer others wash us cleansing us discovery new wave came work directors like wan jen edward yang day first central created new language young taiwanese directors even first attempts speak language hesitant faltering later works refined techniques yang first explored giving taiwan distinctive international presence day long frustrating document nations attempt find voice lack ambition urban life modern taiwan manner antonioni alienation westernized whitecollar middleclass feel lives shaped oldfashioned career men loyal find unfulfilling jiali sylvia chang focus film brings others husband brother friends college charts one happy jiali married husband welei young age time brother urged arranged marriage despite affection another woman jialis marriage seemed comparison good one chose welei free loved goes wrong day presents clich welei becomes absorbed career indulges affair coworker jiali feels restless trapped bound choice made young longer seems wise film never redeems clich conveying feel authentic experience intensity lived pain much seems trite unfolds awkwardly series flashbacks told luncheon vienna jiali meets brothers old flame teresa hu concert pianist seen since college women seem rather sad resigned sadness circumstances jialis brother college friend suggest much made wrong wrong man chosen wrong career path etc rather choices available inadequate jiali married wrong man didnt problem needed marry marry would still unhappy woman modern taiwan day tells us cards stacked dice loaded play game house always wins films sympathetic feminist implications presumably reason caused controversy upon release taiwan seen seem provocative indeed hard imagine anyone felt passionate enough film generate controversy may ambitious may innovative also terribly terribly dull new taiwan necessarily new rest us yang adopts methods established years antonioni others use methods fumbling uncertain bring anything material moments day takes emotional richness strives young couples first kiss awkward meeting woman husbands brief often yang devotes needless time mundane shopping nothing happens little said emotion imparted perhaps film content focus reflective inward moments leaving us guess thoughts feelings might ambiguous suggestive jiali disaffected companions brood silence talk problems length detail eliminate subtleties two hours fortyfive minutes seems long longwinded consider defining moment jiali titular day beach thrust whole film explains easily enough think moment means walks away particular situation even film one two characters explain us verbally case may missed point needless time devoted expressing remained problem throughout look film tedious drawnout narrative first feature shot christopher least name rightly regarded one worlds leading cinematographers freewheeling expressionism work directors like chen wong seems incapable boring image would never know watching day due either doyles inexperience humdrum direction almost perverse insistence making physical environment seem drab banal possible long decay marriage day manages capture ennui experience none damage none heartbreak', 'jet li busted onto american action movie scene stole show lethal weapon wicked looks nasty moves undeniable charisma took another two years joel silver set allamerican movie take empty seats left alleged cokehead van damme pudgy guy named steven seagal would film take li past rival asian action counterparts namely chow yunfat jackie chan lets find plot rival chinese black gangster organizations fall favor one another members respective families start turning dead thats badass jet li blasts picture find men behind brothers death exact style revenge critique three words enough action simple enough enough hong kong kickass jet li action compensate horribly predictable screenplay bad actors crappy dialogue ohso many overthetop melodramatic moments romance angle one must ask love fight scenes really dig jet li little ditty barely contained three memorable action sequences jet well poor dude barely movie thought supposed big break bout giving opportunities show us kungfu fighting chops slap scenes give chances practice abilities letdown even black mask entertaining empty shell film fact sure time spent gathering hits soundtrack film socalled screenplay felt like watching soundtrack rather movie time sad part bad script wouldnt matter much film actually decent actors spouting tacky lines save lindo aaliyah werent shabby thought rest cast picked primarily inability deliver lines convincingly deal whole nfl franchise deal run old looking guy acting like hes drug ring mess dont want seem like im complaining solely story completely foreseeable since go see movies action anyway story well guess thats disappointed film couple cool fight scenes li simply enough satisfy overall craving also much love way incorporate wires special effects stunts couple exaggerated fight scenes simply obvious stunt seamless fellas necessarily laws gravity one cool thing original movie way director showed us inside human body penetrated blow simple creative touch couldnt save rest films uninteresting plot movements im disappointed jet li film didnt give real opportunity star good story many great action scenes hopefully next time charm charismatic actor hope sake films title isnt premonition movies ultimate fate boxoffice']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25768\n",
      "Maximum length: 1317\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 1317, 100)         2576800   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 1310, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 655, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 20960)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                209610    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 2,812,053\n",
      "Trainable params: 2,812,053\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      " - 8s - loss: 0.6915 - acc: 0.4983\n",
      "Epoch 2/10\n",
      " - 7s - loss: 0.6412 - acc: 0.6511\n",
      "Epoch 3/10\n",
      " - 8s - loss: 0.2533 - acc: 0.9300\n",
      "Epoch 4/10\n",
      " - 10s - loss: 0.0193 - acc: 0.9989\n",
      "Epoch 5/10\n",
      " - 10s - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 10s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 10s - loss: 8.4266e-04 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 12s - loss: 5.9825e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 14s - loss: 4.6178e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 16s - loss: 3.6794e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "# define the model\n",
    "def define_model(vocab_size, max_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Conv1D(32, 8, activation='relu'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile network\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "# load training data\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "print(train_docs[:10])\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "# define model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](picture4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 25768\n",
      "Maximum length: 1317\n",
      "Test Accuracy: 86.50\n",
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: NEGATIVE (51.282%)\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (58.544%)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import string\n",
    "import re\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "# open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # prepare regex for char filtering\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # filter out tokens not in vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_train):\n",
    "    documents = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # skip any reviews in the test set\n",
    "        if is_train and filename.startswith('cv9'):\n",
    "            continue\n",
    "        if not is_train and not filename.startswith('cv9'):\n",
    "            continue\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        # clean doc\n",
    "        tokens = clean_doc(doc, vocab)\n",
    "        # add to list\n",
    "        documents.append(tokens)\n",
    "    return documents\n",
    "\n",
    "\n",
    "# load and clean a dataset\n",
    "def load_clean_dataset(vocab, is_train):\n",
    "    # load documents\n",
    "    neg = process_docs('txt_sentoken/neg', vocab, is_train)\n",
    "    pos = process_docs('txt_sentoken/pos', vocab, is_train)\n",
    "    docs = neg + pos\n",
    "    # prepare labels\n",
    "    labels = array([0 for _ in range(len(neg))] + [1 for _ in range(len(pos))])\n",
    "    return docs, labels\n",
    "\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "# integer encode and pad documents\n",
    "def encode_docs(tokenizer, max_length, docs):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(docs)\n",
    "    # pad sequences\n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
    "    return padded\n",
    "\n",
    "\n",
    "# classify a review as negative or positive\n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model):\n",
    "    # clean review\n",
    "    line = clean_doc(review, vocab)\n",
    "    # encode and pad review\n",
    "    padded = encode_docs(tokenizer, max_length, [line])\n",
    "    # predict sentiment\n",
    "    yhat = model.predict(padded, verbose=0)\n",
    "    # retrieve predicted percentage and label\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos) == 0:\n",
    "        return (1-percent_pos), 'NEGATIVE'\n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = set(vocab.split())\n",
    "\n",
    "# load all reviews\n",
    "train_docs, ytrain = load_clean_dataset(vocab, True)\n",
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "# create the tokenizer\n",
    "tokenizer = create_tokenizer(train_docs)\n",
    "# define vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size)\n",
    "# calculate the maximum sequence length\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "print('Maximum length: %d' % max_length)\n",
    "# encode data\n",
    "# Xtrain = encode_docs(tokenizer, max_length, train_docs)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs)\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# evaluate model on training dataset\n",
    "# _, acc = model.evaluate(Xtrain, ytrain, verbose=0)\n",
    "# print('Train Accuracy: %.2f' % (acc*100))\n",
    "# evaluate model on test dataset\n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %.2f' % (acc*100))\n",
    "# test positive text\n",
    "text = 'Everyone will enjoy this film. I love it, recommended!'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n",
    "\n",
    "# test negative text\n",
    "text = 'This is a bad movie. Do not watch it. It sucks.'\n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model)\n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "\n",
    "This section lists some ideas for extending the tutorial that you may wish to explore.\n",
    "\n",
    " Data Cleaning. Explore better data cleaning, perhaps leaving some punctuation in tact\n",
    "or normalizing contractions.\n",
    "\n",
    " Truncated Sequences. Padding all sequences to the length of the longest sequence\n",
    "might be extreme if the longest sequence is very di\u000b",
    "erent to all other reviews. Study the\n",
    "distribution of review lengths and truncate reviews to a mean length.\n",
    "\n",
    " Truncated Vocabulary. We removed infrequently occurring words, but still had a large\n",
    "vocabulary of more than 25,000 words. Explore further reducing the size of the vocabulary\n",
    "and the effect on model skill.\n",
    "\n",
    " Filters and Kernel Size. The number of filters and kernel size are important to model\n",
    "skill and were not tuned. Explore tuning these two CNN parameters.\n",
    "\n",
    " Epochs and Batch Size. The model appears to fitt the training dataset quickly. Explore\n",
    "alternate configurations of the number of training epochs and batch size and use the test\n",
    "dataset as a validation set to pick a better stopping point for training the model.\n",
    "15.7. Further Reading 172\n",
    "\n",
    "\n",
    " Deeper Network. Explore whether a deeper network results in better skill, either in\n",
    "terms of CNN layers, MLP layers and both.\n",
    "\n",
    " Pre-Train an Embedding. Explore pre-training a Word2Vec word embedding in the\n",
    "model and the impact on model skill with and without further fine tuning during training.\n",
    "\n",
    " Use GloVe Embedding. Explore loading the pre-trained GloVe embedding and the\n",
    "impact on model skill with and without further fine tuning during training.\n",
    "\n",
    " Longer Test Reviews. Explore whether the skill of model predictions is dependent on\n",
    "the length of movie reviews as suspected in the final section on evaluating the model.\n",
    "\n",
    " Train Final Model. Train a final model on all available data and use it make predictions\n",
    "on real ad hoc movie reviews from the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
